{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffb391a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch:\n",
      "CUDA available: True\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce MX130\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Check PyTorch GPU\n",
    "print(\"PyTorch:\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ADD THESE 2 LINES AT THE VERY TOP\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "#just add .to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47e29e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "import pyproj\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb9a8e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.read_csv('../DataSets/clean_divar_data.csv')\n",
    "raw_df = pd.read_csv('../DataSets/Divar.csv', usecols=['location_latitude','location_longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb7236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['location_latitude'] = raw_df['location_latitude']\n",
    "clean_df['location_longitude'] = raw_df['location_longitude']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c4fca4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining rows after geographic filtering: 655608\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remove rows with invalid geographic coordinates (global bounds)\n",
    "valid_geo_mask = (\n",
    "    clean_df['location_latitude'].between(-90, 90) & \n",
    "    clean_df['location_longitude'].between(-180, 180)\n",
    ")\n",
    "df = clean_df[valid_geo_mask].copy()\n",
    "print(f\"Remaining rows after geographic filtering: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e367db8",
   "metadata": {},
   "source": [
    "\n",
    "# =====================================================\n",
    "# PART 1: Feature Selection and Initial Analysis\n",
    "# ====================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "299f1b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 highly correlated feature pairs:\n",
      "\n",
      "transformed_credit          transformed_credit_log               0.841922\n",
      "rent_mode                   price_mode                           0.819376\n",
      "building_to_land_ratio      building_size_transformed            0.816034\n",
      "land_size                   unused_land                          0.815260\n",
      "transformable_price         transformable_price_per_sqm          0.783874\n",
      "total_floors_count          total_floors_count_transformed       0.775387\n",
      "credit_value_log            transformable_credit_log             0.773962\n",
      "has_parking                 comfort_score                        0.754902\n",
      "floor                       floor_ratio                          0.751240\n",
      "credit_value                transformed_credit_log               0.746018\n",
      "extra_person_capacity       extra_person_capacity_transformed    0.740458\n",
      "credit_value                credit_value_log                     0.739847\n",
      "user_type                   has_water                            0.727788\n",
      "is_top_floor                is_middle_floor                      0.727742\n",
      "rent_price_on_special_days  rent_price_at_weekends               0.709357\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Only numerical features\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = df[numerical_features].corr().abs()\n",
    "\n",
    "# Select the upper triangle (no duplicate pairs)\n",
    "upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find top 15 highest correlations\n",
    "top_corr = upper_triangle.stack().sort_values(ascending=False).head(15)\n",
    "print(\"Top 15 highly correlated feature pairs:\\n\")\n",
    "print(top_corr)\n",
    "\n",
    "# Optional: visualize as a heatmap\n",
    "top_features = list(set([i for pair in top_corr.index for i in pair]))  # unique features from top pairs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79715323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['has_parking',\n",
       " 'floor_ratio',\n",
       " 'is_middle_floor',\n",
       " 'rent_mode',\n",
       " 'total_floors_count_transformed',\n",
       " 'rent_price_at_weekends',\n",
       " 'land_size',\n",
       " 'extra_person_capacity_transformed',\n",
       " 'transformable_price_per_sqm',\n",
       " 'floor',\n",
       " 'credit_value',\n",
       " 'total_floors_count',\n",
       " 'rent_price_on_special_days',\n",
       " 'transformable_credit_log',\n",
       " 'extra_person_capacity',\n",
       " 'building_size_transformed',\n",
       " 'has_water',\n",
       " 'is_top_floor',\n",
       " 'transformed_credit',\n",
       " 'comfort_score',\n",
       " 'credit_value_log',\n",
       " 'transformed_credit_log',\n",
       " 'transformable_price',\n",
       " 'user_type',\n",
       " 'building_to_land_ratio',\n",
       " 'price_mode',\n",
       " 'unused_land']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e778967",
   "metadata": {},
   "source": [
    "# =====================================================\n",
    "# PART 2: Convert Geographic Coordinates to UTM\n",
    "# ====================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef03723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lat/lon to UTM coordinates\n",
    "# Iran is primarily in UTM Zone 39N and 40N, we'll use 39N for most of the country\n",
    "transformer = pyproj.Transformer.from_crs('EPSG:4326', 'EPSG:32639', always_xy=True)\n",
    "\n",
    "utm_coords = []\n",
    "for idx, row in top_features.iterrows():\n",
    "    try:\n",
    "        x, y = transformer.transform(row['location_longitude'], row['location_latitude'])\n",
    "        utm_coords.append([x, y])\n",
    "    except:\n",
    "        utm_coords.append([np.nan, np.nan])\n",
    "\n",
    "utm_coords = np.array(utm_coords)\n",
    "top_features['utm_x'] = utm_coords[:, 0]\n",
    "top_features['utm_y'] = utm_coords[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e354bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with invalid UTM coordinates\n",
    "df_clustering = df_clustering.dropna(subset=['utm_x', 'utm_y'])\n",
    "\n",
    "print(f\"Dataset shape after UTM conversion: {df_clustering.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86390132",
   "metadata": {},
   "source": [
    "# =====================================================\n",
    "# PART 3: K-Means with 10 Clusters (Initial Analysis)\n",
    "# =====================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6207f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare features for clustering (excluding original lat/lon)\n",
    "features_for_kmeans = df_clustering.drop(['location_latitude', 'location_longitude'], axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(features_for_kmeans)\n",
    "\n",
    "# Apply K-means with 10 clusters\n",
    "kmeans_10 = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
    "clusters_10 = kmeans_10.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df_clustering['cluster_10'] = clusters_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcbf96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot for price vs UTM coordinates\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Create subplot layout\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot 1: UTM coordinates colored by cluster\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "for i in range(10):\n",
    "    cluster_data = df_clustering[df_clustering['cluster_10'] == i]\n",
    "    ax1.scatter(cluster_data['utm_x'], cluster_data['utm_y'], \n",
    "               c=[colors[i]], label=f'Cluster {i}', alpha=0.6, s=30)\n",
    "\n",
    "# Plot cluster centers in UTM space\n",
    "utm_x_idx = list(features_for_kmeans.columns).index('utm_x')\n",
    "utm_y_idx = list(features_for_kmeans.columns).index('utm_y')\n",
    "centers_utm_x = scaler.inverse_transform(kmeans_10.cluster_centers_)[:, utm_x_idx]\n",
    "centers_utm_y = scaler.inverse_transform(kmeans_10.cluster_centers_)[:, utm_y_idx]\n",
    "\n",
    "ax1.scatter(centers_utm_x, centers_utm_y, c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "ax1.set_xlabel('UTM X (meters)')\n",
    "ax1.set_ylabel('UTM Y (meters)')\n",
    "ax1.set_title('K-Means Clustering (k=10) - Geographic Distribution')\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd4e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Price vs UTM X coordinate colored by cluster\n",
    "price_idx = list(features_for_kmeans.columns).index('transformable_price')\n",
    "centers_price = scaler.inverse_transform(kmeans_10.cluster_centers_)[:, price_idx]\n",
    "\n",
    "for i in range(10):\n",
    "    cluster_data = df_clustering[df_clustering['cluster_10'] == i]\n",
    "    ax2.scatter(cluster_data['utm_x'], cluster_data['transformable_price'], \n",
    "               c=[colors[i]], label=f'Cluster {i}', alpha=0.6, s=30)\n",
    "\n",
    "ax2.scatter(centers_utm_x, centers_price, c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "ax2.set_xlabel('UTM X (meters)')\n",
    "ax2.set_ylabel('Transformable Price')\n",
    "ax2.set_title('K-Means Clustering (k=10) - Price vs Location')\n",
    "ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb678cee",
   "metadata": {},
   "source": [
    "# =====================================================\n",
    "# PART 4: Optimal K Selection with Multiple Methods\n",
    "# =====================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5764baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test k values from 1 to 20\n",
    "k_range = range(1, 21)\n",
    "wcss = []  # Within-cluster sum of squares\n",
    "silhouette_scores = []\n",
    "distortions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab8dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing metrics for different k values...\")\n",
    "for k in k_range:\n",
    "    if k == 1:\n",
    "        wcss.append(np.sum((X_scaled - X_scaled.mean(axis=0)) ** 2))\n",
    "        silhouette_scores.append(0)\n",
    "        distortions.append(np.sum((X_scaled - X_scaled.mean(axis=0)) ** 2))\n",
    "    else:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X_scaled)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "        silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "    \n",
    "    print(f\"k={k} completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc7f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Elbow Method\n",
    "ax1.plot(k_range, wcss, 'bo-')\n",
    "ax1.set_xlabel('Number of clusters (k)')\n",
    "ax1.set_ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "ax1.set_title('Elbow Method for Optimal k')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Calculate the rate of change to find elbow\n",
    "wcss_diff = np.diff(wcss)\n",
    "wcss_diff2 = np.diff(wcss_diff)\n",
    "ax2.plot(range(2, len(wcss_diff2)+2), wcss_diff2, 'ro-')\n",
    "ax2.set_xlabel('Number of clusters (k)')\n",
    "ax2.set_ylabel('Second Derivative of WCSS')\n",
    "ax2.set_title('Second Derivative Method')\n",
    "ax2.grid(True)\n",
    "\n",
    "# Silhouette Analysis\n",
    "ax3.plot(k_range[1:], silhouette_scores[1:], 'go-')  # Skip k=1\n",
    "ax3.set_xlabel('Number of clusters (k)')\n",
    "ax3.set_ylabel('Silhouette Score')\n",
    "ax3.set_title('Silhouette Analysis')\n",
    "ax3.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7099065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gap Statistic (simplified version)\n",
    "# Generate random data with same dimensions for comparison\n",
    "np.random.seed(42)\n",
    "random_data = np.random.uniform(X_scaled.min(), X_scaled.max(), X_scaled.shape)\n",
    "wcss_random = []\n",
    "\n",
    "for k in k_range:\n",
    "    if k == 1:\n",
    "        wcss_random.append(np.sum((random_data - random_data.mean(axis=0)) ** 2))\n",
    "    else:\n",
    "        kmeans_random = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans_random.fit(random_data)\n",
    "        wcss_random.append(kmeans_random.inertia_)\n",
    "\n",
    "gap_stats = np.log(wcss_random) - np.log(wcss)\n",
    "ax4.plot(k_range, gap_stats, 'mo-')\n",
    "ax4.set_xlabel('Number of clusters (k)')\n",
    "ax4.set_ylabel('Gap Statistic')\n",
    "ax4.set_title('Gap Statistic Method')\n",
    "ax4.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c014c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find optimal k using different methods\n",
    "optimal_k_silhouette = k_range[1:][np.argmax(silhouette_scores[1:])] + 1  # +1 because we skip k=1\n",
    "optimal_k_gap = k_range[np.argmax(gap_stats)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c529b4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Elbow method - find the point with maximum curvature\n",
    "if len(wcss_diff2) > 0:\n",
    "    optimal_k_elbow = np.argmax(wcss_diff2) + 2  # +2 because of double diff\n",
    "else:\n",
    "    optimal_k_elbow = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d10b1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\\nOptimal k suggestions:\")\n",
    "print(f\"Silhouette method: k = {optimal_k_silhouette}\")\n",
    "print(f\"Gap statistic method: k = {optimal_k_gap}\")\n",
    "print(f\"Elbow method (curvature): k = {optimal_k_elbow}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c210f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select the most frequent suggestion or use domain knowledge\n",
    "suggested_ks = [optimal_k_silhouette, optimal_k_gap, optimal_k_elbow]\n",
    "optimal_k = max(set(suggested_ks), key=suggested_ks.count)\n",
    "print(f\"\\nSelected optimal k: {optimal_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a583ecb4",
   "metadata": {},
   "source": [
    "# =====================================================\n",
    "# PART 5: DBSCAN Clustering\n",
    "# =====================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ab1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use only UTM coordinates and transformable price for DBSCAN\n",
    "dbscan_features = df_clustering[['utm_x', 'utm_y', 'transformable_price']].copy()\n",
    "\n",
    "# Standardize features for DBSCAN\n",
    "scaler_dbscan = StandardScaler()\n",
    "X_dbscan_scaled = scaler_dbscan.fit_transform(dbscan_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e869ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Grid search for optimal DBSCAN parameters to get 3 meaningful clusters\n",
    "eps_values = np.arange(0.1, 2.0, 0.1)\n",
    "min_samples_values = range(5, 50, 5)\n",
    "\n",
    "best_score = -1\n",
    "best_params = {'eps': 0.5, 'min_samples': 10}\n",
    "best_n_clusters = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a57cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Grid search for optimal DBSCAN parameters...\")\n",
    "print(\"Looking for parameters that produce exactly 3 clusters...\")\n",
    "\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(X_dbscan_scaled)\n",
    "        \n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise = list(labels).count(-1)\n",
    "        \n",
    "        # We want exactly 3 clusters with reasonable noise level\n",
    "        if n_clusters == 3 and n_noise < len(labels) * 0.3:  # Less than 30% noise\n",
    "            try:\n",
    "                score = silhouette_score(X_dbscan_scaled, labels)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params = {'eps': eps, 'min_samples': min_samples}\n",
    "                    best_n_clusters = n_clusters\n",
    "                    print(f\"Better params found: eps={eps:.1f}, min_samples={min_samples}, \"\n",
    "                          f\"clusters={n_clusters}, noise={n_noise}, silhouette={score:.3f}\")\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "print(f\"\\nBest DBSCAN parameters: eps={best_params['eps']}, min_samples={best_params['min_samples']}\")\n",
    "print(f\"Best silhouette score: {best_score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c09ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply DBSCAN with best parameters\n",
    "dbscan_final = DBSCAN(eps=best_params['eps'], min_samples=best_params['min_samples'])\n",
    "dbscan_labels = dbscan_final.fit_predict(X_dbscan_scaled)\n",
    "\n",
    "# Add DBSCAN labels to dataframe\n",
    "df_clustering['dbscan_cluster'] = dbscan_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a29d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count clusters and noise points\n",
    "n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise = list(dbscan_labels).count(-1)\n",
    "\n",
    "print(f\"Number of clusters: {n_clusters_dbscan}\")\n",
    "print(f\"Number of noise points: {n_noise}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e295d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DBSCAN results\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "# Plot 1: UTM coordinates\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "unique_labels = set(dbscan_labels)\n",
    "colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Noise points in black\n",
    "        col = [0, 0, 0, 1]\n",
    "        marker = 'x'\n",
    "        label = 'Noise'\n",
    "        size = 20\n",
    "    else:\n",
    "        marker = 'o'\n",
    "        label = f'Cluster {k}'\n",
    "        size = 30\n",
    "\n",
    "    class_member_mask = (dbscan_labels == k)\n",
    "    xy = X_dbscan_scaled[class_member_mask]\n",
    "    \n",
    "    # Convert back to original scale for plotting\n",
    "    xy_original = scaler_dbscan.inverse_transform(xy)\n",
    "    \n",
    "    plt.scatter(xy_original[:, 0], xy_original[:, 1], \n",
    "               c=[col], marker=marker, s=size, alpha=0.6, label=label)\n",
    "\n",
    "plt.xlabel('UTM X (meters)')\n",
    "plt.ylabel('UTM Y (meters)')\n",
    "plt.title(f'DBSCAN Clustering (eps={best_params[\"eps\"]}, min_samples={best_params[\"min_samples\"]})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Price vs UTM X\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        col = [0, 0, 0, 1]\n",
    "        marker = 'x'\n",
    "        label = 'Noise'\n",
    "        size = 20\n",
    "    else:\n",
    "        marker = 'o'\n",
    "        label = f'Cluster {k}'\n",
    "        size = 30\n",
    "\n",
    "    class_member_mask = (dbscan_labels == k)\n",
    "    cluster_data = df_clustering[class_member_mask]\n",
    "    \n",
    "    plt.scatter(cluster_data['utm_x'], cluster_data['transformable_price'], \n",
    "               c=[col], marker=marker, s=size, alpha=0.6, label=label)\n",
    "\n",
    "plt.xlabel('UTM X (meters)')\n",
    "plt.ylabel('Transformable Price')\n",
    "plt.title('DBSCAN Clustering - Price vs Location')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca41641",
   "metadata": {},
   "source": [
    "# =====================================================\n",
    "# HYPERPARAMETER ANALYSIS FOR DBSCAN\n",
    "# =====================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2c110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n=== DBSCAN HYPERPARAMETER ANALYSIS ===\")\n",
    "\n",
    "print(\"DBSCAN Hyperparameter Effects:\")\n",
    "print(\"1. EPS (epsilon):\")\n",
    "print(\"   - Controls the maximum distance between two samples to be considered neighbors\")\n",
    "print(\"   - Smaller eps: More clusters, more noise points\")\n",
    "print(\"   - Larger eps: Fewer clusters, points merge into larger clusters\")\n",
    "print(f\"   - Selected eps: {best_params['eps']}\")\n",
    "\n",
    "print(\"\\n2. MIN_SAMPLES:\")\n",
    "print(\"   - Minimum number of samples in a neighborhood for a point to be considered core point\")\n",
    "print(\"   - Smaller min_samples: More core points, potentially more small clusters\")\n",
    "print(\"   - Larger min_samples: Fewer core points, more noise, denser clusters required\")\n",
    "print(f\"   - Selected min_samples: {best_params['min_samples']}\")\n",
    "\n",
    "print(f\"\\nWith our selected parameters (eps={best_params['eps']}, min_samples={best_params['min_samples']}):\")\n",
    "print(f\"- We achieved {n_clusters_dbscan} clusters as requested\")\n",
    "print(f\"- Noise ratio: {n_noise/len(dbscan_labels)*100:.1f}%\")\n",
    "print(f\"- Silhouette score: {best_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca3666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Summary statistics for each cluster\n",
    "print(\"\\n=== CLUSTER SUMMARY STATISTICS ===\")\n",
    "\n",
    "# K-means clusters summary\n",
    "print(\"K-means Clusters (k=10) Summary:\")\n",
    "for i in range(10):\n",
    "    cluster_data = df_clustering[df_clustering['cluster_10'] == i]\n",
    "    print(f\"Cluster {i}: {len(cluster_data)} properties\")\n",
    "    print(f\"  - Avg Price: {cluster_data['transformable_price'].mean():,.0f}\")\n",
    "    print(f\"  - Avg Building Size: {cluster_data['building_size'].mean():.0f} m²\")\n",
    "    print(f\"  - Avg Rooms: {cluster_data['rooms_count'].mean():.1f}\")\n",
    "\n",
    "print(f\"\\nDBSCAN Clusters Summary:\")\n",
    "for label in set(dbscan_labels):\n",
    "    if label != -1:  # Skip noise\n",
    "        cluster_data = df_clustering[df_clustering['dbscan_cluster'] == label]\n",
    "        print(f\"Cluster {label}: {len(cluster_data)} properties\")\n",
    "        print(f\"  - Avg Price: {cluster_data['transformable_price'].mean():,.0f}\")\n",
    "        print(f\"  - Avg Building Size: {cluster_data['building_size'].mean():.0f} m²\")\n",
    "        print(f\"  - Geographic spread: UTM X range {cluster_data['utm_x'].max()-cluster_data['utm_x'].min():,.0f}m\")\n",
    "\n",
    "if n_noise > 0:\n",
    "    noise_data = df_clustering[df_clustering['dbscan_cluster'] == -1]\n",
    "    print(f\"Noise points: {len(noise_data)} properties\")\n",
    "    print(f\"  - These are outlier properties that don't fit well into any cluster\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f7a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n=== ANALYSIS COMPLETE ===\")\n",
    "print(\"The clustering analysis has identified meaningful property segments based on:\")\n",
    "print(\"- Geographic location (UTM coordinates)\")\n",
    "print(\"- Property characteristics (size, rooms, amenities)\")\n",
    "print(\"- Price levels\")\n",
    "print(\"This segmentation can be used for recommendation systems to suggest similar properties to users.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quera-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
