{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e29e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb9a8e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('your_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e367db8",
   "metadata": {},
   "source": [
    "\n",
    "# =====================================================\n",
    "# PART 1: Feature Selection and Initial Analysis\n",
    "# ====================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be6b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_features = [\n",
    "    'transformable_price',  # Price (transformed for comparison)\n",
    "    'building_size',        # Building area\n",
    "    'rooms_count',          # Number of rooms\n",
    "    'has_elevator',         # Elevator (comfort feature)\n",
    "    'has_parking',          # Parking (essential amenity)\n",
    "    'has_balcony',          # Balcony (lifestyle feature)\n",
    "    'construction_year',    # Age of building\n",
    "    'location_latitude',    # Geographic location\n",
    "    'location_longitude'    # Geographic location\n",
    "]\n",
    "\n",
    "# Create a clean dataset for clustering\n",
    "df_clustering = df[clustering_features].copy()\n",
    "\n",
    "# Handle missing values\n",
    "print(\"Missing values before cleaning:\")\n",
    "print(df_clustering.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e778967",
   "metadata": {},
   "source": [
    "# =====================================================\n",
    "# PART 2: Convert Geographic Coordinates to UTM\n",
    "# ====================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef03723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lat/lon to UTM coordinates\n",
    "# Iran is primarily in UTM Zone 39N and 40N, we'll use 39N for most of the country\n",
    "transformer = pyproj.Transformer.from_crs('EPSG:4326', 'EPSG:32639', always_xy=True)\n",
    "\n",
    "utm_coords = []\n",
    "for idx, row in df_clustering.iterrows():\n",
    "    try:\n",
    "        x, y = transformer.transform(row['location_longitude'], row['location_latitude'])\n",
    "        utm_coords.append([x, y])\n",
    "    except:\n",
    "        utm_coords.append([np.nan, np.nan])\n",
    "\n",
    "utm_coords = np.array(utm_coords)\n",
    "df_clustering['utm_x'] = utm_coords[:, 0]\n",
    "df_clustering['utm_y'] = utm_coords[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e354bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with invalid UTM coordinates\n",
    "df_clustering = df_clustering.dropna(subset=['utm_x', 'utm_y'])\n",
    "\n",
    "print(f\"Dataset shape after UTM conversion: {df_clustering.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86390132",
   "metadata": {},
   "source": [
    "# =====================================================\n",
    "# PART 3: K-Means with 10 Clusters (Initial Analysis)\n",
    "# =====================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6207f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare features for clustering (excluding original lat/lon)\n",
    "features_for_kmeans = df_clustering.drop(['location_latitude', 'location_longitude'], axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(features_for_kmeans)\n",
    "\n",
    "# Apply K-means with 10 clusters\n",
    "kmeans_10 = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
    "clusters_10 = kmeans_10.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df_clustering['cluster_10'] = clusters_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcbf96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot for price vs UTM coordinates\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Create subplot layout\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot 1: UTM coordinates colored by cluster\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "for i in range(10):\n",
    "    cluster_data = df_clustering[df_clustering['cluster_10'] == i]\n",
    "    ax1.scatter(cluster_data['utm_x'], cluster_data['utm_y'], \n",
    "               c=[colors[i]], label=f'Cluster {i}', alpha=0.6, s=30)\n",
    "\n",
    "# Plot cluster centers in UTM space\n",
    "utm_x_idx = list(features_for_kmeans.columns).index('utm_x')\n",
    "utm_y_idx = list(features_for_kmeans.columns).index('utm_y')\n",
    "centers_utm_x = scaler.inverse_transform(kmeans_10.cluster_centers_)[:, utm_x_idx]\n",
    "centers_utm_y = scaler.inverse_transform(kmeans_10.cluster_centers_)[:, utm_y_idx]\n",
    "\n",
    "ax1.scatter(centers_utm_x, centers_utm_y, c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "ax1.set_xlabel('UTM X (meters)')\n",
    "ax1.set_ylabel('UTM Y (meters)')\n",
    "ax1.set_title('K-Means Clustering (k=10) - Geographic Distribution')\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd4e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Price vs UTM X coordinate colored by cluster\n",
    "price_idx = list(features_for_kmeans.columns).index('transformable_price')\n",
    "centers_price = scaler.inverse_transform(kmeans_10.cluster_centers_)[:, price_idx]\n",
    "\n",
    "for i in range(10):\n",
    "    cluster_data = df_clustering[df_clustering['cluster_10'] == i]\n",
    "    ax2.scatter(cluster_data['utm_x'], cluster_data['transformable_price'], \n",
    "               c=[colors[i]], label=f'Cluster {i}', alpha=0.6, s=30)\n",
    "\n",
    "ax2.scatter(centers_utm_x, centers_price, c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "ax2.set_xlabel('UTM X (meters)')\n",
    "ax2.set_ylabel('Transformable Price')\n",
    "ax2.set_title('K-Means Clustering (k=10) - Price vs Location')\n",
    "ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb678cee",
   "metadata": {},
   "source": [
    "# =====================================================\n",
    "# PART 4: Optimal K Selection with Multiple Methods\n",
    "# =====================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5764baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test k values from 1 to 20\n",
    "k_range = range(1, 21)\n",
    "wcss = []  # Within-cluster sum of squares\n",
    "silhouette_scores = []\n",
    "distortions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab8dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing metrics for different k values...\")\n",
    "for k in k_range:\n",
    "    if k == 1:\n",
    "        wcss.append(np.sum((X_scaled - X_scaled.mean(axis=0)) ** 2))\n",
    "        silhouette_scores.append(0)\n",
    "        distortions.append(np.sum((X_scaled - X_scaled.mean(axis=0)) ** 2))\n",
    "    else:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X_scaled)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "        silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "    \n",
    "    print(f\"k={k} completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc7f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Elbow Method\n",
    "ax1.plot(k_range, wcss, 'bo-')\n",
    "ax1.set_xlabel('Number of clusters (k)')\n",
    "ax1.set_ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "ax1.set_title('Elbow Method for Optimal k')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Calculate the rate of change to find elbow\n",
    "wcss_diff = np.diff(wcss)\n",
    "wcss_diff2 = np.diff(wcss_diff)\n",
    "ax2.plot(range(2, len(wcss_diff2)+2), wcss_diff2, 'ro-')\n",
    "ax2.set_xlabel('Number of clusters (k)')\n",
    "ax2.set_ylabel('Second Derivative of WCSS')\n",
    "ax2.set_title('Second Derivative Method')\n",
    "ax2.grid(True)\n",
    "\n",
    "# Silhouette Analysis\n",
    "ax3.plot(k_range[1:], silhouette_scores[1:], 'go-')  # Skip k=1\n",
    "ax3.set_xlabel('Number of clusters (k)')\n",
    "ax3.set_ylabel('Silhouette Score')\n",
    "ax3.set_title('Silhouette Analysis')\n",
    "ax3.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7099065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gap Statistic (simplified version)\n",
    "# Generate random data with same dimensions for comparison\n",
    "np.random.seed(42)\n",
    "random_data = np.random.uniform(X_scaled.min(), X_scaled.max(), X_scaled.shape)\n",
    "wcss_random = []\n",
    "\n",
    "for k in k_range:\n",
    "    if k == 1:\n",
    "        wcss_random.append(np.sum((random_data - random_data.mean(axis=0)) ** 2))\n",
    "    else:\n",
    "        kmeans_random = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans_random.fit(random_data)\n",
    "        wcss_random.append(kmeans_random.inertia_)\n",
    "\n",
    "gap_stats = np.log(wcss_random) - np.log(wcss)\n",
    "ax4.plot(k_range, gap_stats, 'mo-')\n",
    "ax4.set_xlabel('Number of clusters (k)')\n",
    "ax4.set_ylabel('Gap Statistic')\n",
    "ax4.set_title('Gap Statistic Method')\n",
    "ax4.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c014c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find optimal k using different methods\n",
    "optimal_k_silhouette = k_range[1:][np.argmax(silhouette_scores[1:])] + 1  # +1 because we skip k=1\n",
    "optimal_k_gap = k_range[np.argmax(gap_stats)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c529b4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Elbow method - find the point with maximum curvature\n",
    "if len(wcss_diff2) > 0:\n",
    "    optimal_k_elbow = np.argmax(wcss_diff2) + 2  # +2 because of double diff\n",
    "else:\n",
    "    optimal_k_elbow = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d10b1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\\nOptimal k suggestions:\")\n",
    "print(f\"Silhouette method: k = {optimal_k_silhouette}\")\n",
    "print(f\"Gap statistic method: k = {optimal_k_gap}\")\n",
    "print(f\"Elbow method (curvature): k = {optimal_k_elbow}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c210f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select the most frequent suggestion or use domain knowledge\n",
    "suggested_ks = [optimal_k_silhouette, optimal_k_gap, optimal_k_elbow]\n",
    "optimal_k = max(set(suggested_ks), key=suggested_ks.count)\n",
    "print(f\"\\nSelected optimal k: {optimal_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a583ecb4",
   "metadata": {},
   "source": [
    "# =====================================================\n",
    "# PART 5: DBSCAN Clustering\n",
    "# =====================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ab1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use only UTM coordinates and transformable price for DBSCAN\n",
    "dbscan_features = df_clustering[['utm_x', 'utm_y', 'transformable_price']].copy()\n",
    "\n",
    "# Standardize features for DBSCAN\n",
    "scaler_dbscan = StandardScaler()\n",
    "X_dbscan_scaled = scaler_dbscan.fit_transform(dbscan_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e869ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Grid search for optimal DBSCAN parameters to get 3 meaningful clusters\n",
    "eps_values = np.arange(0.1, 2.0, 0.1)\n",
    "min_samples_values = range(5, 50, 5)\n",
    "\n",
    "best_score = -1\n",
    "best_params = {'eps': 0.5, 'min_samples': 10}\n",
    "best_n_clusters = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a57cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Grid search for optimal DBSCAN parameters...\")\n",
    "print(\"Looking for parameters that produce exactly 3 clusters...\")\n",
    "\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(X_dbscan_scaled)\n",
    "        \n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise = list(labels).count(-1)\n",
    "        \n",
    "        # We want exactly 3 clusters with reasonable noise level\n",
    "        if n_clusters == 3 and n_noise < len(labels) * 0.3:  # Less than 30% noise\n",
    "            try:\n",
    "                score = silhouette_score(X_dbscan_scaled, labels)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params = {'eps': eps, 'min_samples': min_samples}\n",
    "                    best_n_clusters = n_clusters\n",
    "                    print(f\"Better params found: eps={eps:.1f}, min_samples={min_samples}, \"\n",
    "                          f\"clusters={n_clusters}, noise={n_noise}, silhouette={score:.3f}\")\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "print(f\"\\nBest DBSCAN parameters: eps={best_params['eps']}, min_samples={best_params['min_samples']}\")\n",
    "print(f\"Best silhouette score: {best_score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c09ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply DBSCAN with best parameters\n",
    "dbscan_final = DBSCAN(eps=best_params['eps'], min_samples=best_params['min_samples'])\n",
    "dbscan_labels = dbscan_final.fit_predict(X_dbscan_scaled)\n",
    "\n",
    "# Add DBSCAN labels to dataframe\n",
    "df_clustering['dbscan_cluster'] = dbscan_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a29d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count clusters and noise points\n",
    "n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise = list(dbscan_labels).count(-1)\n",
    "\n",
    "print(f\"Number of clusters: {n_clusters_dbscan}\")\n",
    "print(f\"Number of noise points: {n_noise}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e295d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DBSCAN results\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "# Plot 1: UTM coordinates\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "unique_labels = set(dbscan_labels)\n",
    "colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Noise points in black\n",
    "        col = [0, 0, 0, 1]\n",
    "        marker = 'x'\n",
    "        label = 'Noise'\n",
    "        size = 20\n",
    "    else:\n",
    "        marker = 'o'\n",
    "        label = f'Cluster {k}'\n",
    "        size = 30\n",
    "\n",
    "    class_member_mask = (dbscan_labels == k)\n",
    "    xy = X_dbscan_scaled[class_member_mask]\n",
    "    \n",
    "    # Convert back to original scale for plotting\n",
    "    xy_original = scaler_dbscan.inverse_transform(xy)\n",
    "    \n",
    "    plt.scatter(xy_original[:, 0], xy_original[:, 1], \n",
    "               c=[col], marker=marker, s=size, alpha=0.6, label=label)\n",
    "\n",
    "plt.xlabel('UTM X (meters)')\n",
    "plt.ylabel('UTM Y (meters)')\n",
    "plt.title(f'DBSCAN Clustering (eps={best_params[\"eps\"]}, min_samples={best_params[\"min_samples\"]})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Price vs UTM X\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        col = [0, 0, 0, 1]\n",
    "        marker = 'x'\n",
    "        label = 'Noise'\n",
    "        size = 20\n",
    "    else:\n",
    "        marker = 'o'\n",
    "        label = f'Cluster {k}'\n",
    "        size = 30\n",
    "\n",
    "    class_member_mask = (dbscan_labels == k)\n",
    "    cluster_data = df_clustering[class_member_mask]\n",
    "    \n",
    "    plt.scatter(cluster_data['utm_x'], cluster_data['transformable_price'], \n",
    "               c=[col], marker=marker, s=size, alpha=0.6, label=label)\n",
    "\n",
    "plt.xlabel('UTM X (meters)')\n",
    "plt.ylabel('Transformable Price')\n",
    "plt.title('DBSCAN Clustering - Price vs Location')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca41641",
   "metadata": {},
   "source": [
    "# =====================================================\n",
    "# HYPERPARAMETER ANALYSIS FOR DBSCAN\n",
    "# =====================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2c110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n=== DBSCAN HYPERPARAMETER ANALYSIS ===\")\n",
    "\n",
    "print(\"DBSCAN Hyperparameter Effects:\")\n",
    "print(\"1. EPS (epsilon):\")\n",
    "print(\"   - Controls the maximum distance between two samples to be considered neighbors\")\n",
    "print(\"   - Smaller eps: More clusters, more noise points\")\n",
    "print(\"   - Larger eps: Fewer clusters, points merge into larger clusters\")\n",
    "print(f\"   - Selected eps: {best_params['eps']}\")\n",
    "\n",
    "print(\"\\n2. MIN_SAMPLES:\")\n",
    "print(\"   - Minimum number of samples in a neighborhood for a point to be considered core point\")\n",
    "print(\"   - Smaller min_samples: More core points, potentially more small clusters\")\n",
    "print(\"   - Larger min_samples: Fewer core points, more noise, denser clusters required\")\n",
    "print(f\"   - Selected min_samples: {best_params['min_samples']}\")\n",
    "\n",
    "print(f\"\\nWith our selected parameters (eps={best_params['eps']}, min_samples={best_params['min_samples']}):\")\n",
    "print(f\"- We achieved {n_clusters_dbscan} clusters as requested\")\n",
    "print(f\"- Noise ratio: {n_noise/len(dbscan_labels)*100:.1f}%\")\n",
    "print(f\"- Silhouette score: {best_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca3666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Summary statistics for each cluster\n",
    "print(\"\\n=== CLUSTER SUMMARY STATISTICS ===\")\n",
    "\n",
    "# K-means clusters summary\n",
    "print(\"K-means Clusters (k=10) Summary:\")\n",
    "for i in range(10):\n",
    "    cluster_data = df_clustering[df_clustering['cluster_10'] == i]\n",
    "    print(f\"Cluster {i}: {len(cluster_data)} properties\")\n",
    "    print(f\"  - Avg Price: {cluster_data['transformable_price'].mean():,.0f}\")\n",
    "    print(f\"  - Avg Building Size: {cluster_data['building_size'].mean():.0f} m²\")\n",
    "    print(f\"  - Avg Rooms: {cluster_data['rooms_count'].mean():.1f}\")\n",
    "\n",
    "print(f\"\\nDBSCAN Clusters Summary:\")\n",
    "for label in set(dbscan_labels):\n",
    "    if label != -1:  # Skip noise\n",
    "        cluster_data = df_clustering[df_clustering['dbscan_cluster'] == label]\n",
    "        print(f\"Cluster {label}: {len(cluster_data)} properties\")\n",
    "        print(f\"  - Avg Price: {cluster_data['transformable_price'].mean():,.0f}\")\n",
    "        print(f\"  - Avg Building Size: {cluster_data['building_size'].mean():.0f} m²\")\n",
    "        print(f\"  - Geographic spread: UTM X range {cluster_data['utm_x'].max()-cluster_data['utm_x'].min():,.0f}m\")\n",
    "\n",
    "if n_noise > 0:\n",
    "    noise_data = df_clustering[df_clustering['dbscan_cluster'] == -1]\n",
    "    print(f\"Noise points: {len(noise_data)} properties\")\n",
    "    print(f\"  - These are outlier properties that don't fit well into any cluster\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f7a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n=== ANALYSIS COMPLETE ===\")\n",
    "print(\"The clustering analysis has identified meaningful property segments based on:\")\n",
    "print(\"- Geographic location (UTM coordinates)\")\n",
    "print(\"- Property characteristics (size, rooms, amenities)\")\n",
    "print(\"- Price levels\")\n",
    "print(\"This segmentation can be used for recommendation systems to suggest similar properties to users.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quera-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
