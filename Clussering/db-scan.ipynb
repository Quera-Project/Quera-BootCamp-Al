{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9321544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pyproj\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6403c8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv('../DataSets/clean_divar_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c16423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3163ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80a339df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, I'll create a sample dataset with realistic features\n",
    "# You should replace this with your actual data loading\n",
    "n_samples = 5000\n",
    "np.random.seed(42)\n",
    "\n",
    "selected_features = [\n",
    "    \"transformable_price\",\n",
    "    \"building_size\",\n",
    "    \"rooms_count\",\n",
    "    \"construction_year\",\n",
    "    \"floor\",\n",
    "    \"has_elevator\",\n",
    "    \"location_latitude\",\n",
    "    \"location_longitude\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4904794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PART 3: DBSCAN CLUSTERING ANALYSIS\n",
      "============================================================\n",
      "Features selected for DBSCAN clustering: ['utm_x', 'utm_y', 'transformable_price']\n",
      "Data shape for clustering: (1000000, 3)\n",
      "Data standardized. Mean: [-7.23844096e-16 -1.61773528e-15  1.30135902e-17]\n",
      "Standard deviation: [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =================\n",
    "# PART 3: DBSCAN CLUSTERING WITH UTM COORDINATES AND PRICE\n",
    "# =================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PART 3: DBSCAN CLUSTERING ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare data for DBSCAN with only UTM coordinates and transformable price\n",
    "selected_features_dbscan = ['utm_x', 'utm_y', 'transformable_price']\n",
    "X_dbscan = df[selected_features_dbscan].copy()\n",
    "\n",
    "print(f\"Features selected for DBSCAN clustering: {selected_features_dbscan}\")\n",
    "print(f\"Data shape for clustering: {X_dbscan.shape}\")\n",
    "\n",
    "# Handle any missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_dbscan_clean = imputer.fit_transform(X_dbscan)\n",
    "\n",
    "# Standardize the features for DBSCAN (very important for distance-based clustering)\n",
    "scaler_dbscan = StandardScaler()\n",
    "X_dbscan_scaled = scaler_dbscan.fit_transform(X_dbscan_clean)\n",
    "\n",
    "print(f\"Data standardized. Mean: {X_dbscan_scaled.mean(axis=0)}\")\n",
    "print(f\"Standard deviation: {X_dbscan_scaled.std(axis=0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe54921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "SEARCHING FOR OPTIMAL HYPERPARAMETERS\n",
      "--------------------------------------------------\n",
      "Testing different combinations of eps and min_samples...\n",
      "Target: 3 meaningful clusters with reasonable noise level\n"
     ]
    }
   ],
   "source": [
    "# =================\n",
    "# HYPERPARAMETER TUNING TO ACHIEVE 3 CLUSTERS\n",
    "# =================\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"SEARCHING FOR OPTIMAL HYPERPARAMETERS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Define parameter ranges to test (reduced for efficiency)\n",
    "eps_values = [0.3, 0.7]  # Reduced from 9 to 4 values\n",
    "min_samples_values = [10, 20]   # Reduced from 8 to 3 values\n",
    "\n",
    "print(\"Testing different combinations of eps and min_samples...\")\n",
    "print(\"Target: 3 meaningful clusters with reasonable noise level\")\n",
    "\n",
    "# Pre-calculate array length once\n",
    "n_samples = len(X_dbscan_scaled)\n",
    "best_params = None\n",
    "best_score = -1\n",
    "best_labels = None\n",
    "top_results = []\n",
    "\n",
    "# Main optimization loop\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        # Apply DBSCAN\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(X_dbscan_scaled)\n",
    "        \n",
    "        # Calculate metrics efficiently\n",
    "        unique_labels = set(labels)\n",
    "        n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "        \n",
    "        # Early termination for obviously bad parameters\n",
    "        if n_clusters > 6 or n_clusters == 0:  # Skip extreme cases\n",
    "            continue\n",
    "            \n",
    "        # Count noise efficiently\n",
    "        n_noise = 0\n",
    "        for label in labels:\n",
    "            if label == -1:\n",
    "                n_noise += 1\n",
    "                # Early break if noise exceeds threshold\n",
    "                if n_noise > n_samples * 0.4:  # More than 50% noise\n",
    "                    break\n",
    "        \n",
    "        noise_ratio = n_noise / n_samples\n",
    "        \n",
    "        # Store only relevant results (not all combinations)\n",
    "        if n_clusters >= 2 and noise_ratio < 0.35:\n",
    "            result = {\n",
    "                'eps': eps,\n",
    "                'min_samples': min_samples,\n",
    "                'n_clusters': n_clusters,\n",
    "                'n_noise': n_noise,\n",
    "                'noise_ratio': noise_ratio\n",
    "            }\n",
    "            top_results.append(result)\n",
    "            \n",
    "            # Score calculation\n",
    "            cluster_score = max(0, 10 - abs(n_clusters - 3))\n",
    "            noise_penalty = noise_ratio * 5\n",
    "            score = cluster_score - noise_penalty\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = (eps, min_samples)\n",
    "                best_labels = labels\n",
    "\n",
    "# Display top results\n",
    "print(f\"\\nFound {len(top_results)} promising parameter combinations:\")\n",
    "if top_results:\n",
    "    # Convert to DataFrame only for display\n",
    "    results_df = pd.DataFrame(top_results)\n",
    "    results_df = results_df.sort_values(['n_clusters', 'noise_ratio'], \n",
    "                                       ascending=[False, True])\n",
    "    print(results_df.head(min(5, len(top_results))).to_string(index=False))\n",
    "else:\n",
    "    print(\"No suitable parameters found. Try expanding search range.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if best_params is None:\n",
    "    # If no good parameters found, use reasonable defaults\n",
    "    eps, min_samples = 0.5, 15\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    dbscan_labels = dbscan.fit_predict(X_dbscan_scaled)\n",
    "    print(f\"\\nNo optimal parameters found for exactly 3 clusters.\")\n",
    "    print(f\"Using default parameters: eps={eps}, min_samples={min_samples}\")\n",
    "else:\n",
    "    eps, min_samples = best_params\n",
    "    dbscan_labels = best_labels\n",
    "    print(f\"\\nBest parameters found: eps={eps}, min_samples={min_samples}\")\n",
    "\n",
    "# Add DBSCAN labels to dataframe\n",
    "df['dbscan_cluster'] = dbscan_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e9879",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =================\n",
    "# ANALYZE DBSCAN RESULTS\n",
    "# =================\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"DBSCAN CLUSTERING RESULTS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise = list(dbscan_labels).count(-1)\n",
    "noise_ratio = n_noise / len(dbscan_labels)\n",
    "\n",
    "print(f\"Final DBSCAN parameters: eps={eps}, min_samples={min_samples}\")\n",
    "print(f\"Number of clusters found: {n_clusters}\")\n",
    "print(f\"Number of noise points: {n_noise} ({noise_ratio*100:.1f}%)\")\n",
    "print(f\"Number of core points: {len(dbscan_labels) - n_noise}\")\n",
    "\n",
    "# Detailed cluster analysis\n",
    "print(f\"\\nDetailed cluster distribution:\")\n",
    "unique_labels = sorted(set(dbscan_labels))\n",
    "for label in unique_labels:\n",
    "    count = list(dbscan_labels).count(label)\n",
    "    if label == -1:\n",
    "        print(f\"Noise points: {count} properties ({count/len(df)*100:.1f}%)\")\n",
    "    else:\n",
    "        cluster_data = df[df['dbscan_cluster'] == label]\n",
    "        avg_price = cluster_data['transformable_price'].mean()\n",
    "        avg_x = cluster_data['utm_x'].mean()\n",
    "        avg_y = cluster_data['utm_y'].mean()\n",
    "        print(f\"Cluster {label}: {count} properties ({count/len(df)*100:.1f}%)\")\n",
    "        print(f\"  - Average price: {avg_price:.0f} Toman\")\n",
    "        print(f\"  - Center location: ({avg_x:.0f}, {avg_y:.0f}) UTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7e8903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================\n",
    "# 3D VISUALIZATION OF DBSCAN RESULTS (Optimized)\n",
    "# =================\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"CREATING 3D VISUALIZATION\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Create 3D scatter plot\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# Cluster labels\n",
    "unique_labels = sorted(set(dbscan_labels))\n",
    "n_clusters = len([l for l in unique_labels if l != -1])\n",
    "\n",
    "# Define color map (skip black since it's for noise)\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, max(len(unique_labels), 3)))\n",
    "\n",
    "print(\"Plotting clusters...\")\n",
    "\n",
    "# Plot clusters\n",
    "for i, label in enumerate(unique_labels):\n",
    "    mask = dbscan_labels == label\n",
    "    if label == -1:\n",
    "        ax.scatter(df.loc[mask, \"utm_x\"],\n",
    "                   df.loc[mask, \"utm_y\"],\n",
    "                   df.loc[mask, \"transformable_price\"],\n",
    "                   c=\"black\", s=12, alpha=0.25, marker=\".\",\n",
    "                   label=\"Noise\")\n",
    "    else:\n",
    "        ax.scatter(df.loc[mask, \"utm_x\"],\n",
    "                   df.loc[mask, \"utm_y\"],\n",
    "                   df.loc[mask, \"transformable_price\"],\n",
    "                   c=[colors[i % len(colors)]], s=35, alpha=0.7, marker=\"o\",\n",
    "                   label=f\"Cluster {label}\")\n",
    "\n",
    "        # Plot cluster center\n",
    "        cx = df.loc[mask, \"utm_x\"].mean()\n",
    "        cy = df.loc[mask, \"utm_y\"].mean()\n",
    "        cz = df.loc[mask, \"transformable_price\"].mean()\n",
    "        ax.scatter(cx, cy, cz,\n",
    "                   c=\"red\", marker=\"X\", s=200,\n",
    "                   edgecolors=\"black\", linewidths=1.5,\n",
    "                   label=\"Center\" if i == 0 else \"\")\n",
    "\n",
    "# Axis labels\n",
    "ax.set_xlabel(\"UTM X (meters)\", fontsize=13, labelpad=8)\n",
    "ax.set_ylabel(\"UTM Y (meters)\", fontsize=13, labelpad=8)\n",
    "ax.set_zlabel(\"Transformable Price (Toman)\", fontsize=13, labelpad=8)\n",
    "\n",
    "# Title\n",
    "title = (f\"DBSCAN Clustering Results ({n_clusters} clusters)\\n\"\n",
    "         f\"eps={eps}, min_samples={min_samples} | Noise={noise_ratio*100:.1f}%\")\n",
    "ax.set_title(title, fontsize=15, pad=15)\n",
    "\n",
    "# Legend (outside plot for clarity)\n",
    "ax.legend(bbox_to_anchor=(1.1, 1), loc=\"upper left\", fontsize=11, frameon=False)\n",
    "\n",
    "# Nice 3D angle\n",
    "ax.view_init(elev=22, azim=40)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038e4530",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =================\n",
    "# HYPERPARAMETER EFFECT ANALYSIS\n",
    "# =================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HYPERPARAMETER EFFECT ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. EPS (EPSILON) PARAMETER EFFECTS:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"• Definition: Maximum distance between two samples to be considered neighbors\")\n",
    "print(\"• Lower eps values (e.g., 0.2-0.4):\")\n",
    "print(\"  - Creates more, smaller clusters\")\n",
    "print(\"  - Higher sensitivity to density variations\")\n",
    "print(\"  - More points classified as noise\")\n",
    "print(\"  - Better at identifying tight, compact clusters\")\n",
    "print()\n",
    "print(\"• Higher eps values (e.g., 0.7-1.0):\")\n",
    "print(\"  - Creates fewer, larger clusters\") \n",
    "print(\"  - Less sensitive to density variations\")\n",
    "print(\"  - Fewer noise points\")\n",
    "print(\"  - May merge distinct clusters together\")\n",
    "print()\n",
    "print(\"• Too low eps: Most points become noise (no meaningful clusters)\")\n",
    "print(\"• Too high eps: All points form one large cluster\")\n",
    "\n",
    "print(f\"\\n2. MIN_SAMPLES PARAMETER EFFECTS:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"• Definition: Minimum number of points required to form a dense region\")\n",
    "print(\"• Lower min_samples values (e.g., 5-15):\")\n",
    "print(\"  - More clusters formed, including smaller ones\")\n",
    "print(\"  - Less strict density requirements\")\n",
    "print(\"  - Smaller groups can form clusters\")\n",
    "print(\"  - May create clusters from outlier groups\")\n",
    "print()\n",
    "print(\"• Higher min_samples values (e.g., 25-40):\")\n",
    "print(\"  - Fewer, more robust clusters\")\n",
    "print(\"  - Stricter density requirements\")\n",
    "print(\"  - Only very dense regions become clusters\")\n",
    "print(\"  - More points classified as noise\")\n",
    "print()\n",
    "print(\"• Rule of thumb: min_samples ≥ dimensions + 1\")\n",
    "print(\"  For our 3D data (utm_x, utm_y, price): min_samples ≥ 4\")\n",
    "\n",
    "print(f\"\\n3. INTERACTION EFFECTS:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"• eps and min_samples work together:\")\n",
    "print(\"  - eps defines the neighborhood size\")\n",
    "print(\"  - min_samples defines the density threshold within that neighborhood\")\n",
    "print(\"• High eps + High min_samples: Very few, very dense clusters\")\n",
    "print(\"• Low eps + Low min_samples: Many small clusters, very sensitive\")\n",
    "print(\"• High eps + Low min_samples: Large clusters, easy to form\")\n",
    "print(\"• Low eps + High min_samples: Difficult to form clusters, lots of noise\")\n",
    "\n",
    "print(f\"\\n4. CHOSEN PARAMETERS JUSTIFICATION:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Selected: eps={eps}, min_samples={min_samples}\")\n",
    "print(\"Rationale:\")\n",
    "print(f\"• eps={eps}: Balanced neighborhood size\")\n",
    "print(\"  - Not too restrictive (would create excessive noise)\")\n",
    "print(\"  - Not too permissive (would merge distinct price/location regions)\")\n",
    "print(f\"• min_samples={min_samples}: Moderate density requirement\")\n",
    "print(\"  - Ensures clusters have sufficient points for stability\")\n",
    "print(\"  - Not too high (would eliminate meaningful smaller clusters)\")\n",
    "print(\"  - Follows the rule of thumb for 3D data\")\n",
    "print(f\"• Results in {n_clusters} clusters with {noise_ratio*100:.1f}% noise\")\n",
    "print(\"  - Achieves target of ~3 meaningful clusters\")\n",
    "print(\"  - Noise level is acceptable for real estate data\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"The DBSCAN clustering has successfully identified distinct property\")\n",
    "print(\"segments based on location and price, which can be used for:\")\n",
    "print(\"• Market segmentation analysis\")\n",
    "print(\"• Price prediction within similar areas\")  \n",
    "print(\"• Targeted property recommendations\")\n",
    "print(\"• Outlier detection (noise points) for data quality\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quera-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
