{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f3c5686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd764b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data from Part 1\n",
    "try:\n",
    "    df_clustering = pd.read_pickle('../DataSets/clustering_results_part1.pkl')\n",
    "    # Remove any existing cluster columns\n",
    "    df_clustering = df_clustering.drop(['cluster_10'], axis=1, errors='ignore')\n",
    "    print(\"Loaded preprocessed data from Part 1\")\n",
    "except:\n",
    "    print(\"Please run Part 1 first to generate the preprocessed data\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Dataset shape: {df_clustering.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc643bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DBSCAN CLUSTERING ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nAs requested, using only 3 features for DBSCAN:\")\n",
    "print(\"  • utm_x (Geographic X coordinate)\")\n",
    "print(\"  • utm_y (Geographic Y coordinate)\")  \n",
    "print(\"  • transformable_price (Price)\")\n",
    "\n",
    "# Select only UTM coordinates and price for DBSCAN\n",
    "dbscan_features = ['utm_x', 'utm_y', 'transformable_price']\n",
    "df_dbscan = df_clustering[dbscan_features].copy()\n",
    "\n",
    "print(f\"\\nDBSCAN dataset shape: {df_dbscan.shape}\")\n",
    "print(f\"Features summary:\")\n",
    "print(df_dbscan.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734d5bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standardize features for DBSCAN\n",
    "scaler_dbscan = StandardScaler()\n",
    "X_dbscan_scaled = scaler_dbscan.fit_transform(df_dbscan)\n",
    "\n",
    "print(f\"\\nData standardized for DBSCAN clustering\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c2963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================================================\n",
    "# HYPERPARAMETER GRID SEARCH FOR 3 CLUSTERS\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"SEARCHING FOR OPTIMAL DBSCAN PARAMETERS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Goal: Find parameters that produce exactly 3 meaningful clusters\")\n",
    "print(f\"Constraints: Less than 30% noise points\")\n",
    "\n",
    "# Define parameter ranges\n",
    "eps_values = np.arange(0.1, 2.0, 0.1)\n",
    "min_samples_values = range(5, 51, 5)\n",
    "\n",
    "best_score = -1\n",
    "best_params = None\n",
    "best_n_clusters = None\n",
    "best_labels = None\n",
    "results_log = []\n",
    "\n",
    "# For memory efficiency with large datasets\n",
    "max_sample_silhouette = min(1000, len(X_dbscan_scaled))\n",
    "sample_indices = np.random.choice(len(X_dbscan_scaled), max_sample_silhouette, replace=False)\n",
    "\n",
    "print(f\"Testing {len(eps_values)} eps values × {len(min_samples_values)} min_samples values = {len(eps_values) * len(min_samples_values)} combinations\")\n",
    "print(f\"Using sample of {max_sample_silhouette} points for silhouette evaluation\")\n",
    "\n",
    "print(f\"\\nProgress (searching for 3-cluster solutions):\")\n",
    "valid_solutions = 0\n",
    "\n",
    "for i, eps in enumerate(eps_values):\n",
    "    for j, min_samples in enumerate(min_samples_values):\n",
    "        # Apply DBSCAN\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=1)\n",
    "        labels = dbscan.fit_predict(X_dbscan_scaled)\n",
    "        \n",
    "        # Calculate cluster statistics\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise = np.sum(labels == -1)\n",
    "        noise_ratio = n_noise / len(labels)\n",
    "        \n",
    "        # Only consider solutions with exactly 3 clusters and reasonable noise\n",
    "        if n_clusters == 3 and noise_ratio < 0.3:\n",
    "            try:\n",
    "                # Calculate silhouette score on sample\n",
    "                labels_sample = labels[sample_indices]\n",
    "                X_sample = X_dbscan_scaled[sample_indices]\n",
    "                \n",
    "                # Only calculate if we have all clusters represented in sample\n",
    "                unique_labels_sample = set(labels_sample)\n",
    "                if len(unique_labels_sample) >= 2 and -1 not in unique_labels_sample:\n",
    "                    score = silhouette_score(X_sample, labels_sample)\n",
    "                    \n",
    "                    # Store result\n",
    "                    result = {\n",
    "                        'eps': eps,\n",
    "                        'min_samples': min_samples,\n",
    "                        'n_clusters': n_clusters,\n",
    "                        'n_noise': n_noise,\n",
    "                        'noise_ratio': noise_ratio,\n",
    "                        'silhouette_score': score\n",
    "                    }\n",
    "                    results_log.append(result)\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_params = {\"eps\": eps, \"min_samples\": min_samples}\n",
    "                        best_n_clusters = n_clusters\n",
    "                        best_labels = labels.copy()\n",
    "                        \n",
    "                        print(f\"  ✓ eps={eps:.1f}, min_samples={min_samples:2d} → \"\n",
    "                              f\"clusters={n_clusters}, noise={n_noise:4d}({noise_ratio*100:.1f}%), \"\n",
    "                              f\"silhouette={score:.3f}\")\n",
    "                        \n",
    "                    valid_solutions += 1\n",
    "                        \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    # Progress indicator\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"    Completed {i+1}/{len(eps_values)} eps values... ({valid_solutions} valid solutions found)\")\n",
    "\n",
    "print(f\"\\nGrid search completed!\")\n",
    "print(f\"Total valid solutions found: {valid_solutions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ff3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if best_params is None:\n",
    "    print(\"⚠️  No suitable parameters found for exactly 3 clusters with <30% noise\")\n",
    "    print(\"Relaxing constraints to find best available solution...\")\n",
    "    \n",
    "    # Fallback: find best solution with any number of clusters\n",
    "    best_score = -1\n",
    "    for eps in eps_values[::2]:  # Sample fewer values\n",
    "        for min_samples in min_samples_values[::2]:\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            labels = dbscan.fit_predict(X_dbscan_scaled)\n",
    "            \n",
    "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "            if n_clusters >= 2:\n",
    "                try:\n",
    "                    score = silhouette_score(X_dbscan_scaled[sample_indices], \n",
    "                                           labels[sample_indices])\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_params = {\"eps\": eps, \"min_samples\": min_samples}\n",
    "                        best_n_clusters = n_clusters\n",
    "                        best_labels = labels.copy()\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    print(f\"Fallback solution: eps={best_params['eps']}, min_samples={best_params['min_samples']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ea1333",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================================================\n",
    "# FINAL DBSCAN APPLICATION\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL DBSCAN RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Best parameters found:\")\n",
    "print(f\"  • eps = {best_params['eps']}\")\n",
    "print(f\"  • min_samples = {best_params['min_samples']}\")\n",
    "print(f\"  • Number of clusters: {best_n_clusters}\")\n",
    "print(f\"  • Silhouette score: {best_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973cddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply final DBSCAN with best parameters\n",
    "dbscan_final = DBSCAN(eps=best_params['eps'], min_samples=best_params['min_samples'])\n",
    "final_labels = dbscan_final.fit_predict(X_dbscan_scaled)\n",
    "\n",
    "# Add DBSCAN results to dataframe\n",
    "df_dbscan['dbscan_cluster'] = final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d071046",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate final statistics\n",
    "n_clusters_final = len(set(final_labels)) - (1 if -1 in final_labels else 0)\n",
    "n_noise_final = np.sum(final_labels == -1)\n",
    "noise_ratio_final = n_noise_final / len(final_labels)\n",
    "\n",
    "print(f\"\\nFinal clustering statistics:\")\n",
    "print(f\"  • Total data points: {len(final_labels)}\")\n",
    "print(f\"  • Number of clusters: {n_clusters_final}\")\n",
    "print(f\"  • Noise points: {n_noise_final} ({noise_ratio_final*100:.1f}%)\")\n",
    "print(f\"  • Clustered points: {len(final_labels) - n_noise_final} ({(1-noise_ratio_final)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1737bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cluster size distribution\n",
    "print(f\"\\nCluster size distribution:\")\n",
    "unique_labels = set(final_labels)\n",
    "for label in sorted(unique_labels):\n",
    "    if label == -1:\n",
    "        print(f\"  • Noise: {np.sum(final_labels == label)} points\")\n",
    "    else:\n",
    "        print(f\"  • Cluster {label}: {np.sum(final_labels == label)} points\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50df29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================================================\n",
    "# VISUALIZATIONS\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\nGenerating DBSCAN visualizations...\")\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "# Generate colors for clusters\n",
    "unique_labels = set(final_labels)\n",
    "colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "# Plot 1: UTM coordinates colored by cluster\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Noise points\n",
    "        col = [0, 0, 0, 1]  # Black\n",
    "        marker = 'x'\n",
    "        label = f'Noise ({np.sum(final_labels == k)} pts)'\n",
    "        size = 15\n",
    "        alpha = 0.4\n",
    "    else:\n",
    "        marker = 'o'\n",
    "        label = f'Cluster {k} ({np.sum(final_labels == k)} pts)'\n",
    "        size = 30\n",
    "        alpha = 0.7\n",
    "\n",
    "    class_member_mask = (final_labels == k)\n",
    "    cluster_data = df_dbscan[class_member_mask]\n",
    "    \n",
    "    ax1.scatter(cluster_data['utm_x'], cluster_data['utm_y'], \n",
    "               c=[col], marker=marker, s=size, alpha=alpha, label=label, edgecolors='white', linewidth=0.5)\n",
    "\n",
    "ax1.set_xlabel('UTM X (meters)', fontsize=12)\n",
    "ax1.set_ylabel('UTM Y (meters)', fontsize=12)\n",
    "ax1.set_title(f'DBSCAN Geographic Distribution\\n(eps={best_params[\"eps\"]}, min_samples={best_params[\"min_samples\"]})', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Price vs UTM X colored by cluster\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        col = [0, 0, 0, 1]\n",
    "        marker = 'x'\n",
    "        size = 15\n",
    "        alpha = 0.4\n",
    "    else:\n",
    "        marker = 'o'\n",
    "        size = 30\n",
    "        alpha = 0.7\n",
    "\n",
    "    class_member_mask = (final_labels == k)\n",
    "    cluster_data = df_dbscan[class_member_mask]\n",
    "    \n",
    "    ax2.scatter(cluster_data['utm_x'], cluster_data['transformable_price'], \n",
    "               c=[col], marker=marker, s=size, alpha=alpha, edgecolors='white', linewidth=0.5)\n",
    "\n",
    "ax2.set_xlabel('UTM X (meters)', fontsize=12)\n",
    "ax2.set_ylabel('Transformable Price', fontsize=12)\n",
    "ax2.set_title('DBSCAN: Price vs Geographic Location (X)', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Price vs UTM Y colored by cluster\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        col = [0, 0, 0, 1]\n",
    "        marker = 'x'\n",
    "        size = 15\n",
    "        alpha = 0.4\n",
    "    else:\n",
    "        marker = 'o'\n",
    "        size = 30\n",
    "        alpha = 0.7\n",
    "\n",
    "    class_member_mask = (final_labels == k)\n",
    "    cluster_data = df_dbscan[class_member_mask]\n",
    "    \n",
    "    ax3.scatter(cluster_data['utm_y'], cluster_data['transformable_price'], \n",
    "               c=[col], marker=marker, s=size, alpha=alpha, edgecolors='white', linewidth=0.5)\n",
    "\n",
    "ax3.set_xlabel('UTM Y (meters)', fontsize=12)\n",
    "ax3.set_ylabel('Transformable Price', fontsize=12)\n",
    "ax3.set_title('DBSCAN: Price vs Geographic Location (Y)', fontsize=14, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: 3D representation (UTM X vs UTM Y, sized by price)\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        col = [0, 0, 0, 1]\n",
    "        marker = 'x'\n",
    "        alpha = 0.3\n",
    "        size_factor = 10\n",
    "    else:\n",
    "        marker = 'o'\n",
    "        alpha = 0.6\n",
    "        size_factor = 50\n",
    "\n",
    "    class_member_mask = (final_labels == k)\n",
    "    cluster_data = df_dbscan[class_member_mask]\n",
    "    \n",
    "    # Size points by price (normalized)\n",
    "    if len(cluster_data) > 0:\n",
    "        price_sizes = (cluster_data['transformable_price'] - cluster_data['transformable_price'].min() + 1)\n",
    "        price_sizes = price_sizes / price_sizes.max() * size_factor + 10\n",
    "        \n",
    "        ax4.scatter(cluster_data['utm_x'], cluster_data['utm_y'], \n",
    "                   c=[col], marker=marker, s=price_sizes, alpha=alpha, \n",
    "                   edgecolors='white', linewidth=0.5)\n",
    "\n",
    "ax4.set_xlabel('UTM X (meters)', fontsize=12)\n",
    "ax4.set_ylabel('UTM Y (meters)', fontsize=12)\n",
    "ax4.set_title('DBSCAN: Geographic Distribution\\n(Point size = Price)', fontsize=14, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a009d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "====================================================\n",
    "# HYPERPARAMETER ANALYSIS EXPLANATION\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"DBSCAN HYPERPARAMETER ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"📘 DBSCAN Hyperparameter Effects:\")\n",
    "print(f\"\\n1. EPS (ε - Epsilon):\")\n",
    "print(f\"   • Definition: Maximum distance between two samples to be considered neighbors\")\n",
    "print(f\"   • Effect on clustering:\")\n",
    "print(f\"     - Smaller eps (e.g., 0.1-0.5): More clusters, tighter groups, more noise\")\n",
    "print(f\"     - Larger eps (e.g., 1.0-2.0): Fewer, larger clusters, less noise\")\n",
    "print(f\"   • Selected eps: {best_params['eps']}\")\n",
    "print(f\"   • Interpretation: Points within {best_params['eps']} standard deviations are neighbors\")\n",
    "\n",
    "print(f\"\\n2. MIN_SAMPLES:\")\n",
    "print(f\"   • Definition: Minimum points required in a neighborhood to form a core point\")\n",
    "print(f\"   • Effect on clustering:\")\n",
    "print(f\"     - Smaller min_samples (e.g., 5-10): More core points, smaller clusters possible\")\n",
    "print(f\"     - Larger min_samples (e.g., 20-50): Fewer core points, denser clusters required\")\n",
    "print(f\"   • Selected min_samples: {best_params['min_samples']}\")\n",
    "print(f\"   • Interpretation: Need at least {best_params['min_samples']} points to form cluster core\")\n",
    "\n",
    "print(f\"\\n3. COMBINED EFFECT:\")\n",
    "print(f\"   • Our parameters (eps={best_params['eps']}, min_samples={best_params['min_samples']}):\")\n",
    "print(f\"     - Created {n_clusters_final} distinct clusters\")\n",
    "print(f\"     - Identified {n_noise_final} outlier properties ({noise_ratio_final*100:.1f}%)\")\n",
    "print(f\"     - Achieved silhouette score of {best_score:.3f}\")\n",
    "\n",
    "print(f\"\\n4. BUSINESS INTERPRETATION:\")\n",
    "print(f\"   • Clusters represent property groups with similar:\")\n",
    "print(f\"     - Geographic proximity (UTM coordinates)\")\n",
    "print(f\"     - Price ranges (transformable_price)\")\n",
    "print(f\"   • Noise points are unique/outlier properties that don't fit standard patterns\")\n",
    "print(f\"   • Useful for recommendation: suggest properties from same cluster\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9de0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# CLUSTER ANALYSIS\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"DETAILED CLUSTER ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Cluster characteristics:\")\n",
    "for label in sorted(unique_labels):\n",
    "    if label == -1:\n",
    "        cluster_data = df_dbscan[final_labels == label]\n",
    "        print(f\"\\n🔴 NOISE POINTS ({len(cluster_data)} properties):\")\n",
    "        print(f\"   • These are outlier properties that don't fit into any cluster\")\n",
    "        print(f\"   • Average price: {cluster_data['transformable_price'].mean():,.0f}\")\n",
    "        print(f\"   • Price range: {cluster_data['transformable_price'].min():,.0f} - {cluster_data['transformable_price'].max():,.0f}\")\n",
    "        print(f\"   • Geographic spread:\")\n",
    "        print(f\"     - UTM X range: {cluster_data['utm_x'].max() - cluster_data['utm_x'].min():,.0f} meters\")\n",
    "        print(f\"     - UTM Y range: {cluster_data['utm_y'].max() - cluster_data['utm_y'].min():,.0f} meters\")\n",
    "    else:\n",
    "        cluster_data = df_dbscan[final_labels == label]\n",
    "        print(f\"\\n🟢 CLUSTER {label} ({len(cluster_data)} properties):\")\n",
    "        print(f\"   • Average price: {cluster_data['transformable_price'].mean():,.0f}\")\n",
    "        print(f\"   • Price std: {cluster_data['transformable_price'].std():,.0f}\")\n",
    "        print(f\"   • Price range: {cluster_data['transformable_price'].min():,.0f} - {cluster_data['transformable_price'].max():,.0f}\")\n",
    "        print(f\"   • Geographic center: UTM({cluster_data['utm_x'].mean():.0f}, {cluster_data['utm_y'].mean():.0f})\")\n",
    "        print(f\"   • Geographic spread:\")\n",
    "        print(f\"     - UTM X range: {cluster_data['utm_x'].max() - cluster_data['utm_x'].min():,.0f} meters\")\n",
    "        print(f\"     - UTM Y range: {cluster_data['utm_y'].max() - cluster_data['utm_y'].min():,.0f} meters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a20178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================================================\n",
    "# PARAMETER SENSITIVITY ANALYSIS\n",
    "# =====================================================\n",
    "\n",
    "if len(results_log) > 0:\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"PARAMETER SENSITIVITY ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Convert results to DataFrame for analysis\n",
    "    results_df = pd.DataFrame(results_log)\n",
    "    \n",
    "    print(f\"Top 5 parameter combinations (by silhouette score):\")\n",
    "    top_results = results_df.nlargest(5, 'silhouette_score')\n",
    "    print(f\"{'Rank':<4} {'eps':<5} {'min_samples':<12} {'clusters':<9} {'noise%':<8} {'silhouette':<11}\")\n",
    "    print(\"-\" * 55)\n",
    "    for i, (_, row) in enumerate(top_results.iterrows(), 1):\n",
    "        print(f\"{i:<4} {row['eps']:<5.1f} {row['min_samples']:<12} {row['n_clusters']:<9} \"\n",
    "              f\"{row['noise_ratio']*100:<8.1f} {row['silhouette_score']:<11.3f}\")\n",
    "    \n",
    "    # Parameter distribution analysis\n",
    "    print(f\"\\nParameter ranges that produced 3 clusters:\")\n",
    "    print(f\"  • eps range: {results_df['eps'].min():.1f} - {results_df['eps'].max():.1f}\")\n",
    "    print(f\"  • min_samples range: {results_df['min_samples'].min()} - {results_df['min_samples'].max()}\")\n",
    "    print(f\"  • Average noise ratio: {results_df['noise_ratio'].mean()*100:.1f}%\")\n",
    "    print(f\"  • Average silhouette: {results_df['silhouette_score'].mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b73631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# SAVE RESULTS\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save DBSCAN results\n",
    "dbscan_results = {\n",
    "    'best_params': best_params,\n",
    "    'final_labels': final_labels,\n",
    "    'n_clusters': n_clusters_final,\n",
    "    'n_noise': n_noise_final,\n",
    "    'silhouette_score': best_score,\n",
    "    'df_dbscan': df_dbscan,\n",
    "    'scaler_dbscan': scaler_dbscan\n",
    "}\n",
    "\n",
    "# Save to pickle\n",
    "import pickle\n",
    "with open('dbscan_results.pkl', 'wb') as f:\n",
    "    pickle.dump(dbscan_results, f)\n",
    "\n",
    "# Also save the DataFrame with cluster assignments\n",
    "df_dbscan.to_pickle('../DataSets/dbscan_clustered_data.pkl')\n",
    "\n",
    "\n",
    "print(f\"✅ DBSCAN results saved to:\")\n",
    "print(f\"   • 'dbscan_results.pkl' (complete results)\")\n",
    "print(f\"   • 'dbscan_clustered_data.pkl' (clustered dataset)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2b1397",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\\n🎯 DBSCAN CLUSTERING COMPLETE!\")\n",
    "print(f\"   • Successfully created {n_clusters_final} clusters using geographic location and price\")\n",
    "print(f\"   • Optimal parameters: eps={best_params['eps']}, min_samples={best_params['min_samples']}\")\n",
    "print(f\"   • Quality metric: Silhouette score = {best_score:.3f}\")\n",
    "print(f\"   • Practical outcome: {noise_ratio_final*100:.1f}% outliers identified\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428b17fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\\n📊 For Recommendation System:\")\n",
    "print(f\"   • Users can be recommended properties from the same cluster\")\n",
    "print(f\"   • Clusters represent similar properties in terms of location and price\")\n",
    "print(f\"   • Noise points represent unique properties that might be special deals or unique listings\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quera-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
