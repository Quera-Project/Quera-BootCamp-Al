{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "405wgN5rXTn8",
      "metadata": {
        "id": "405wgN5rXTn8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import (StandardScaler, MinMaxScaler, RobustScaler,\n",
        "                                   PowerTransformer, QuantileTransformer, LabelEncoder)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import KNNImputer, SimpleImputer\n",
        "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import accuracy_score, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from scipy import stats\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from persiantools.jdatetime import JalaliDate\n",
        "from datetime import datetime\n",
        "import lightgbm as lgb\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "6cf27a71",
      "metadata": {},
      "outputs": [],
      "source": [
        "divar_df = pd.read_parquet('Divar.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "Z40I5VR4Y3AF",
      "metadata": {
        "id": "Z40I5VR4Y3AF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original dataset: 1,000,000 rows × 61 columns\n"
          ]
        }
      ],
      "source": [
        "df = divar_df.copy()\n",
        "original_shape = df.shape\n",
        "print(f\"Original dataset: {original_shape[0]:,} rows × {original_shape[1]} columns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "SW57o9ygZCSe",
      "metadata": {
        "id": "SW57o9ygZCSe"
      },
      "outputs": [],
      "source": [
        "# Column definitions for real estate domain\n",
        "target_columns = ['price_value', 'rent_value',\n",
        "                  'credit_value', 'transformable_price']\n",
        "\n",
        "boolean_columns = ['rent_to_single', 'has_business_deed', 'has_balcony',\n",
        "                   'has_elevator', 'has_electricity', 'has_gas', 'has_security_guard',\n",
        "                   'has_warehouse', 'has_parking', 'is_rebuilt', 'has_water',\n",
        "                   'has_barbecue', 'has_pool', 'has_jacuzzi', 'has_sauna',\n",
        "                   'rent_credit_transform', 'transformable_price']\n",
        "numerical_columns = [\n",
        "    'rent_value', 'price_value', 'credit_value',\n",
        "    'transformable_credit', 'transformed_credit', 'transformable_rent', 'transformed_rent',\n",
        "    'land_size', 'building_size', 'floor', 'rooms_count', 'total_floors_count',\n",
        "    'unit_per_floor', 'construction_year', 'regular_person_capacity',\n",
        "    'extra_person_capacity', 'cost_per_extra_person',\n",
        "    'rent_price_on_regular_days', 'rent_price_on_special_days',\n",
        "    'rent_price_at_weekends', 'month_created', 'seaseon_created', 'year_created'\n",
        "]\n",
        "\n",
        "price_columns = ['price_value', 'rent_value', 'credit_value', 'transformable_price',\n",
        "                 'transformable_credit', 'transformed_credit', 'transformable_rent',\n",
        "                 'transformed_rent', 'cost_per_extra_person','rent_price_on_regular_days', \n",
        "                 'rent_price_on_special_days', 'rent_price_at_weekends',]\n",
        "\n",
        "size_columns = ['land_size', 'building_size']\n",
        "\n",
        "location_columns = ['cat2_slug', 'cat3_slug',\n",
        "                    'city_slug', 'neighborhood_slug']\n",
        "\n",
        "one_hot_colums = ['has_cooling_system', 'has_restroom',\n",
        "                  'has_heating_system', 'has_warm_water_provider']\n",
        "\n",
        "categorical_columns = ['user_type', 'rent_mode', 'credit_mode'\n",
        "                       'cat2_slug', 'cat3_slug', 'city_slug', \n",
        "                       'deed_type', 'building_direction','floor_material', \n",
        "                       'property_type', 'neighborhood_slug'\n",
        "                       ]\n",
        "\n",
        "amenity_columns = [\n",
        "    'has_balcony', 'has_elevator', 'has_parking', 'has_warehouse', 'price_mode',\n",
        "    'has_water', 'has_gas', 'has_electricity', 'has_pool', 'has_jacuzzi',\n",
        "    'has_sauna', 'has_barbecue', 'has_security_guard'\n",
        "]\n",
        "\n",
        "property_columns = ['property_type','building_direction',\n",
        "                    'floor_material', 'deed_type']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49ff9ae5",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# STEP 2:  DATA CLEANING AND TRANSFORMATION\n",
        "# ============================================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "ZMO2H04zZMD4",
      "metadata": {
        "id": "ZMO2H04zZMD4"
      },
      "outputs": [],
      "source": [
        "# Drop obviously irrelevant columns\n",
        "drop_cols = ['Unnamed: 0', 'location_radius', 'location_latitude', 'location_longitude', 'title', 'description' ]\n",
        "deleted_colums = df[drop_cols]\n",
        "df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "bMj_EC_eajVy",
      "metadata": {
        "id": "bMj_EC_eajVy"
      },
      "outputs": [],
      "source": [
        "# Persian number conversion function\n",
        "def convert_persian_numbers(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    persian_digits = '۰۱۲۳۴۵۶۷۸۹'\n",
        "    english_digits = '0123456789'\n",
        "    for p, e in zip(persian_digits, english_digits):\n",
        "        text = str(text).replace(p, e)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "oOtuI3NIavh0",
      "metadata": {
        "id": "oOtuI3NIavh0"
      },
      "outputs": [],
      "source": [
        "# Apply Persian conversion to relevant columns\n",
        "persian_cols = ['construction_year', 'total_floors_count', 'floor', 'rooms_count']\n",
        "for col in persian_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].apply(convert_persian_numbers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "ojEu5CX4ayGh",
      "metadata": {
        "id": "ojEu5CX4ayGh"
      },
      "outputs": [],
      "source": [
        "#  rooms count mapping\n",
        "rooms_mapping = {\n",
        "    'بدون اتاق': 0, 'یک': 1, 'دو': 2, 'سه': 3, 'چهار': 4,\n",
        "    'پنج یا بیشتر': 5, '5+': 5, 'استودیو': 0\n",
        "}\n",
        "if 'rooms_count' in df.columns:\n",
        "    df['rooms_count'] = df['rooms_count'].map(rooms_mapping).astype('Int8')\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "ovl65WPmbGRq",
      "metadata": {
        "id": "ovl65WPmbGRq"
      },
      "outputs": [],
      "source": [
        "# Construction year cleaning with validation\n",
        "if 'construction_year' in df.columns:\n",
        "    df['construction_year'] = pd.to_numeric(df['construction_year'].str.extract(r'(\\d+)')[0], errors='coerce')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "7o2rqYW6bLNc",
      "metadata": {
        "id": "7o2rqYW6bLNc"
      },
      "outputs": [],
      "source": [
        "# Floor and building floors validation\n",
        "for col in ['floor', 'total_floors_count', 'extra_person_capacity']:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].replace({'30+': 31, 'more_than_30': 31, 'unselect': 1})\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "clLYP6UrbOwR",
      "metadata": {
        "id": "clLYP6UrbOwR"
      },
      "outputs": [],
      "source": [
        "# Unit per floor cleaning\n",
        "if 'unit_per_floor' in df.columns:\n",
        "    df['unit_per_floor'] = df['unit_per_floor'].replace({\n",
        "        'more_than_8': 9, 'unselect': 1\n",
        "    })\n",
        "    df['unit_per_floor'] = pd.to_numeric(df['unit_per_floor'], errors='coerce')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "Ozg6fE3wbScc",
      "metadata": {
        "id": "Ozg6fE3wbScc"
      },
      "outputs": [],
      "source": [
        "# Persian text mapping for categorical columns\n",
        "persian_mappings = {\n",
        "    'user_type': {'مشاور املاک': 'agent', 'شخصی': 'personal'},\n",
        "    'rent_mode': {'مقطوع': 'fixed', 'مجانی': 'free', 'توافقی': 'negotiable'},\n",
        "    'price_mode': {'مقطوع': 'fixed', 'مجانی': 'free', 'توافقی': 'negotiable'},\n",
        "    'credit_mode': {'مقطوع': 'fixed', 'مجانی': 'free', 'توافقی': 'negotiable'}\n",
        "}\n",
        "\n",
        "for col, mapping in persian_mappings.items():\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].map(mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "q2Cqt3nHbXHN",
      "metadata": {
        "id": "q2Cqt3nHbXHN"
      },
      "outputs": [],
      "source": [
        "# Boolean column standardization\n",
        "bool_map = {True: 1, False: 0, 'true': 1, 'false': 0, 'yes': 1, 'no': 0, 'unselect': 0}\n",
        "for col in boolean_columns:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].map(bool_map).astype('Int8')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8a207ff",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# STEP 3:  OUTLIER DETECTION AND HANDLING\n",
        "# ============================================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "63bb68c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_outliers_multiple_methods(series, methods=['iqr', 'zscore', 'isolation']):\n",
        "    \"\"\"Detect outliers using multiple methods for robust identification\"\"\"\n",
        "    outliers = pd.Series(False, index=series.index)\n",
        "\n",
        "    if 'iqr' in methods:\n",
        "        Q1 = series.quantile(0.25)\n",
        "        Q3 = series.quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        iqr_outliers = (series < Q1 - 2.5 * IQR) | (series > Q3 + 2.5 * IQR)\n",
        "        outliers |= iqr_outliers\n",
        "\n",
        "    if 'zscore' in methods and len(series) > 3:\n",
        "        z_scores = np.abs(stats.zscore(series, nan_policy='omit'))\n",
        "        zscore_outliers = z_scores > 3.5\n",
        "        outliers |= zscore_outliers\n",
        "\n",
        "    if 'isolation' in methods and len(series) > 50:\n",
        "        try:\n",
        "            iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
        "            iso_outliers = iso_forest.fit_predict(series.values.reshape(-1, 1)) == -1\n",
        "            outliers |= iso_outliers\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e607ab8f",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# STEP 4: MISSING VALUE IMPUTATION\n",
        "# ============================================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "M6qNTxPduzY8",
      "metadata": {
        "id": "M6qNTxPduzY8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filling: rent_to_single\n",
            "Validation Accuracy: 100.0\n",
            "Filling: has_business_deed\n",
            "Validation Accuracy: 94.4\n",
            "Filling: has_balcony\n",
            "Validation Accuracy: 91.8\n",
            "Filling: has_elevator\n",
            "Validation Accuracy: 91.5\n",
            "Filling: has_electricity\n",
            "Validation Accuracy: 87.8\n",
            "Filling: has_gas\n",
            "Validation Accuracy: 87.7\n",
            "Filling: has_security_guard\n",
            "Validation Accuracy: 93.2\n",
            "Filling: has_warehouse\n",
            "Validation Accuracy: 81.7\n",
            "Filling: has_parking\n",
            "Validation Accuracy: 83.3\n",
            "Filling: is_rebuilt\n",
            "Validation Accuracy: 83.4\n",
            "Filling: has_water\n",
            "Validation Accuracy: 87.5\n",
            "Filling: has_barbecue\n",
            "Validation Accuracy: 90.1\n",
            "Filling: has_pool\n",
            "Validation Accuracy: 93.9\n",
            "Filling: has_jacuzzi\n",
            "Validation Accuracy: 95.9\n",
            "Filling: has_sauna\n",
            "Validation Accuracy: 97.8\n",
            "Filling: rent_credit_transform\n",
            "Validation Accuracy: 99.8\n",
            "Filling: transformable_price\n",
            "Validation Accuracy: 99.7\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Features for training (exclude boolean targets except first)\n",
        "features = df.drop(columns=boolean_columns).copy()\n",
        "\n",
        "# Encode categorical columns\n",
        "for col in features.select_dtypes('object').columns:\n",
        "    features[col] = features[col].astype('category').cat.codes\n",
        "\n",
        "# Fill numeric NaNs\n",
        "for col in features.select_dtypes('number').columns:\n",
        "    features[col] = features[col].fillna(features[col].median())\n",
        "\n",
        "for target in boolean_columns:\n",
        "    print(f\"Filling: {target}\")\n",
        "    mask_train = df[target].notna()\n",
        "    if mask_train.sum() == 0:\n",
        "        continue\n",
        "\n",
        "    X_train, y_train = features[mask_train], df.loc[mask_train, target].astype(int)\n",
        "    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_data = lgb.Dataset(X_tr, label=y_tr)\n",
        "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
        "\n",
        "    model = lgb.train(\n",
        "        {'objective':'binary','metric':'binary_error','boosting_type':'gbdt',\n",
        "         'learning_rate':0.05,'num_leaves':31,'verbose':-1,'seed':42},\n",
        "        train_data, valid_sets=[val_data]\n",
        "    )\n",
        "\n",
        "    mask_missing = df[target].isna()\n",
        "    if mask_missing.any():\n",
        "        df.loc[mask_missing, target] = (model.predict(features[mask_missing]) > 0.5).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y_val, (model.predict(X_val) > 0.5).astype(int))\n",
        "    print(f\"Validation Accuracy: {acc * 100:.1f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "7dc13eb6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  price_value: 36643 outliers (6.4%) - capped\n",
            "  rent_value: 24184 outliers (6.9%) - capped\n",
            "  credit_value: 18802 outliers (5.3%) - capped\n",
            "  transformable_price: 243276 outliers (24.3%) - capped\n",
            "  transformable_credit: 18802 outliers (5.3%) - capped\n",
            "  transformed_credit: 4375 outliers (6.0%) - capped\n",
            "  transformable_rent: 24183 outliers (6.9%) - capped\n",
            "  transformed_rent: 4038 outliers (5.6%) - capped\n",
            "  cost_per_extra_person: 512 outliers (5.0%) - capped\n",
            "  rent_price_on_regular_days: 802 outliers (4.4%) - capped\n",
            "  rent_price_on_special_days: 524 outliers (5.0%) - capped\n",
            "  rent_price_at_weekends: 611 outliers (4.5%) - capped\n",
            "  land_size: 13287 outliers (7.1%) - capped\n",
            "  building_size: 89075 outliers (9.1%) - capped\n"
          ]
        }
      ],
      "source": [
        "# Apply outlier detection to price and size columns\n",
        "outlier_summary = {}\n",
        "\n",
        "for col in price_columns + size_columns:\n",
        "    if col in df.columns and df[col].notna().sum() > 100:\n",
        "        # Ensure column is numeric\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "        # Work only with valid numeric data\n",
        "        valid_data = df[col].dropna()\n",
        "        if valid_data.empty:\n",
        "            continue  # skip if nothing usable\n",
        "\n",
        "        # Detect outliers\n",
        "        outliers = detect_outliers_multiple_methods(valid_data)\n",
        "        outlier_count = outliers.sum()\n",
        "        outlier_percentage = (outlier_count / len(valid_data)) * 100\n",
        "\n",
        "        # Save summary\n",
        "        outlier_summary[col] = {\n",
        "            'count': outlier_count,\n",
        "            'percentage': outlier_percentage\n",
        "        }\n",
        "\n",
        "        # Cap extreme outliers instead of removing them\n",
        "        if outlier_percentage > 1:  # only if significant\n",
        "            lower_cap = df[col].quantile(0.01)\n",
        "            upper_cap = df[col].quantile(0.99)\n",
        "            df[col] = df[col].clip(lower=lower_cap, upper=upper_cap)\n",
        "\n",
        "        print(f\"  {col}: {outlier_count} outliers ({outlier_percentage:.1f}%) - capped\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "G-_45Ohudvhq",
      "metadata": {
        "id": "G-_45Ohudvhq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Low missing (<5%): 22 columns\n",
            "Medium missing (5-30%): 2 columns\n",
            "High missing (>30%): 31 columns\n"
          ]
        }
      ],
      "source": [
        "# Separate columns by missing value percentage for different strategies\n",
        "missing_analysis = df.isnull().sum() / len(df) * 100\n",
        "low_missing = missing_analysis[missing_analysis <= 5].index.tolist()\n",
        "medium_missing = missing_analysis[(missing_analysis > 5) & (missing_analysis <= 30)].index.tolist()\n",
        "high_missing = missing_analysis[missing_analysis > 30].index.tolist()\n",
        "\n",
        "print(f\"Low missing (<5%): {len(low_missing)} columns\")\n",
        "print(f\"Medium missing (5-30%): {len(medium_missing)} columns\")\n",
        "print(f\"High missing (>30%): {len(high_missing)} columns\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "5c461f9c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns with missing values:\n",
            "                            missing_count  missing_percentage\n",
            "cost_per_extra_person              989759               98.98\n",
            "rent_price_on_special_days         989537               98.95\n",
            "rent_price_at_weekends             986449               98.64\n",
            "rent_price_on_regular_days         981932               98.19\n",
            "extra_person_capacity              975991               97.60\n",
            "property_type                      972943               97.29\n",
            "regular_person_capacity            970130               97.01\n",
            "transformed_credit                 927591               92.76\n",
            "transformed_rent                   927591               92.76\n",
            "rent_type                          896039               89.60\n",
            "land_size                          813604               81.36\n",
            "deed_type                          746542               74.65\n",
            "user_type                          711118               71.11\n",
            "unit_per_floor                     697717               69.77\n",
            "total_floors_count                 695648               69.56\n",
            "building_direction                 676077               67.61\n",
            "has_cooling_system                 649381               64.94\n",
            "transformable_rent                 648752               64.88\n",
            "rent_value                         648678               64.87\n",
            "transformable_credit               647915               64.79\n",
            "credit_value                       647905               64.79\n",
            "credit_mode                        647006               64.70\n",
            "rent_mode                          647006               64.70\n",
            "has_heating_system                 631031               63.10\n",
            "has_warm_water_provider            620500               62.05\n",
            "floor_material                     594016               59.40\n",
            "has_restroom                       593087               59.31\n",
            "neighborhood_slug                  562861               56.29\n",
            "floor                              458252               45.83\n",
            "price_value                        431654               43.17\n",
            "price_mode                         426394               42.64\n",
            "construction_year                  184172               18.42\n",
            "rooms_count                        154101               15.41\n",
            "building_size                       19606                1.96\n",
            "city_slug                               2                0.00\n",
            "cat3_slug                               1                0.00\n",
            "\n",
            "Columns without missing values: 19\n",
            "\n",
            "Total columns: 55\n",
            "Columns with missing values: 36\n",
            "Columns without missing values: 19\n",
            "Overall missing percentage: 42.13%\n"
          ]
        }
      ],
      "source": [
        "# Check missing values for all columns\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
        "\n",
        "# Create missing values summary\n",
        "missing_summary = pd.DataFrame({\n",
        "    'missing_count': missing_values,\n",
        "    'missing_percentage': missing_percentage.round(2)\n",
        "})\n",
        "\n",
        "# Show columns with missing values\n",
        "columns_with_missing = missing_summary[missing_summary['missing_count'] > 0]\n",
        "print(\"Columns with missing values:\")\n",
        "print(columns_with_missing.sort_values('missing_percentage', ascending=False))\n",
        "\n",
        "# Show columns without missing values\n",
        "columns_without_missing = missing_summary[missing_summary['missing_count'] == 0]\n",
        "print(f\"\\nColumns without missing values: {len(columns_without_missing)}\")\n",
        "\n",
        "# Total summary\n",
        "print(f\"\\nTotal columns: {len(df.columns)}\")\n",
        "print(f\"Columns with missing values: {len(columns_with_missing)}\")\n",
        "print(f\"Columns without missing values: {len(columns_without_missing)}\")\n",
        "print(f\"Overall missing percentage: {df.isnull().sum().sum() / (len(df) * len(df.columns)) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "YYxiG6PIiiAc",
      "metadata": {
        "id": "YYxiG6PIiiAc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imputed cat3_slug with mode: apartment-sell\n",
            "Imputed city_slug with mode: tehran\n",
            "Imputed rent_to_single with mean: 1.00\n",
            "Imputed rent_credit_transform with mean: 0.15\n",
            "Imputed transformable_price with median: 0.00\n",
            "Imputed building_size with median: 103.00\n",
            "Imputed has_business_deed with mean: 0.48\n",
            "Imputed has_balcony with mean: 0.83\n",
            "Imputed has_elevator with mean: 0.60\n",
            "Imputed has_warehouse with mean: 0.78\n",
            "Imputed has_parking with mean: 0.75\n",
            "Imputed is_rebuilt with mean: 0.40\n",
            "Imputed has_water with mean: 0.36\n",
            "Imputed has_electricity with mean: 0.35\n",
            "Imputed has_gas with mean: 0.34\n",
            "Imputed has_security_guard with mean: 0.29\n",
            "Imputed has_barbecue with mean: 0.29\n",
            "Imputed has_pool with mean: 0.29\n",
            "Imputed has_jacuzzi with mean: 0.29\n",
            "Imputed has_sauna with mean: 0.29\n"
          ]
        }
      ],
      "source": [
        "# Strategy 1: Simple imputation for low missing\n",
        "# Impute low missing numerical columns\n",
        "continus_numeric = price_columns + size_columns\n",
        "\n",
        "all_numric  = df.select_dtypes(include=['number']).columns.tolist()\n",
        "for col in low_missing:\n",
        "    if col in all_numric:\n",
        "        if col in continus_numeric:  # Continuous price-like columns\n",
        "            # Use median for price columns (robust to outliers)\n",
        "            imputer = SimpleImputer(strategy='median')\n",
        "            df[col] = imputer.fit_transform(df[[col]]).ravel()\n",
        "            print(f\"Imputed {col} with median: {imputer.statistics_[0]:.2f}\")\n",
        "        else:\n",
        "            # Use mean for other numerical columns\n",
        "            imputer = SimpleImputer(strategy='mean')\n",
        "            df[col] = imputer.fit_transform(df[[col]]).ravel()\n",
        "            print(f\"Imputed {col} with mean: {imputer.statistics_[0]:.2f}\")\n",
        "    \n",
        "    elif col in categorical_columns:\n",
        "        # Use mode for categorical columns\n",
        "        imputer = SimpleImputer(strategy='most_frequent')\n",
        "        df[col] = imputer.fit_transform(df[[col]]).ravel()\n",
        "        print(f\"Imputed {col} with mode: {imputer.statistics_[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-mhlE2Hjispb",
      "metadata": {
        "id": "-mhlE2Hjispb"
      },
      "outputs": [],
      "source": [
        "# # Strategy 2: KNN imputation for medium missing numerical columns\n",
        "# medium_numerical = [col for col in medium_missing if col in df.select_dtypes(include=[np.number]).columns]\n",
        "# if medium_numerical:\n",
        "#     print(f\"Applying KNN imputation to {len(medium_numerical)} numerical columns...\")\n",
        "#     knn_imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
        "#     df[medium_numerical] = knn_imputer.fit_transform(df[medium_numerical])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a538ffa5",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# STEP :  CATEGORICAL ENCODING\n",
        "# ============================================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "a69470c3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded 16 categorical columns\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Identify categorical columns\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Target encoding for high-cardinality location columns (if target available)\n",
        "high_card_cols = ['city_slug', 'neighborhood_slug']\n",
        "target_col = 'transformable_price'  # Primary target\n",
        "if target_col in df.columns:\n",
        "    for col in high_card_cols:\n",
        "        if col in df.columns and col in categorical_cols and col not in one_hot_colums:\n",
        "            # Calculate mean target value for each category\n",
        "            target_mean = df.groupby(col)[target_col].mean()\n",
        "            global_mean = df[target_col].mean()\n",
        "\n",
        "            # Add smoothing to prevent overfitting\n",
        "            category_counts = df[col].value_counts()\n",
        "            smoothing = 100  # Smoothing parameter\n",
        "            smoothed_means = (target_mean * category_counts + global_mean * smoothing) / (category_counts + smoothing)\n",
        "\n",
        "            df[f'{col}_target_encoded'] = df[col].map(smoothed_means).fillna(global_mean).astype('float32')\n",
        "            categorical_cols.remove(col) if col in categorical_cols else None\n",
        "            \n",
        "            \n",
        "# Label encoding for remaining categorical columns\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    if col in df.columns:\n",
        "        le = LabelEncoder()\n",
        "        # Handle missing values\n",
        "        df[col] = df[col].astype(str).fillna('Unknown')\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "        label_encoders[col] = le\n",
        "\n",
        "        # Convert to appropriate integer type\n",
        "        if len(le.classes_) < 128:\n",
        "            df[col] = df[col].astype('Int8')\n",
        "        elif len(le.classes_) < 32768:\n",
        "            df[col] = df[col].astype('Int16')\n",
        "\n",
        "print(f\"Encoded {len(categorical_cols)} categorical columns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "19d8bb28",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# One-hot encode specified columns using sklearn\n",
        "one_hot_columns = ['has_cooling_system', 'has_restroom', 'has_heating_system', 'has_warm_water_provider']\n",
        "\n",
        "for col in one_hot_columns:\n",
        "    if col in df.columns:\n",
        "        # Convert to string and handle NaN\n",
        "        col_data = df[col].astype(str)\n",
        "        \n",
        "        # Initialize and fit one-hot encoder\n",
        "        ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "        encoded_data = ohe.fit_transform(col_data.values.reshape(-1, 1))\n",
        "        \n",
        "        # Create column names\n",
        "        feature_names = ohe.get_feature_names_out([col])\n",
        "        \n",
        "        # Create DataFrame with encoded columns\n",
        "        encoded_df = pd.DataFrame(encoded_data, columns=feature_names, index=df.index)\n",
        "        \n",
        "        # Drop original column and concatenate encoded columns\n",
        "        df.drop(col, axis=1, inplace=True)\n",
        "        df = pd.concat([df, encoded_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "506be279",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No object columns left - all data is numeric\n"
          ]
        }
      ],
      "source": [
        "# Check for remaining object columns\n",
        "object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "if object_cols:\n",
        "    print(\"Object columns remaining:\", object_cols)\n",
        "    print(\"Number of object columns:\", len(object_cols))\n",
        "else:\n",
        "    print(\"No object columns left - all data is numeric\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "bf638e74",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Label encode high cardinality columns but keep missing values as NaN\n",
        "high_card_cols = ['city_slug', 'neighborhood_slug']\n",
        "\n",
        "for col in high_card_cols:\n",
        "    # Create temporary copy without missing values for fitting\n",
        "    non_null_mask = df[col].notna()\n",
        "    non_null_values = df.loc[non_null_mask, col]\n",
        "    \n",
        "    # Fit LabelEncoder on non-null values only\n",
        "    le = LabelEncoder()\n",
        "    le.fit(non_null_values.astype(str))\n",
        "    \n",
        "    # Transform only non-null values, keep missing as NaN\n",
        "    df.loc[non_null_mask, col] = le.transform(non_null_values.astype(str))\n",
        "    \n",
        "    # Convert to appropriate integer type\n",
        "    unique_count = len(le.classes_)\n",
        "    if unique_count < 128:\n",
        "        df[col] = df[col].astype('Int8')\n",
        "    elif unique_count < 32768:\n",
        "        df[col] = df[col].astype('Int16')\n",
        "    else:\n",
        "        df[col] = df[col].astype('Int32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "bb6ad550",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<IntegerArray>\n",
              "[ 669,  363, 1121,  274,  283,  675, <NA>,  221,  319,  369,\n",
              " ...\n",
              "   12, 1029,  225, 1080,  148,  890,  870, 1024,  473,  469]\n",
              "Length: 1189, dtype: Int16"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['neighborhood_slug'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2gcB6DkQi6F7",
      "metadata": {
        "id": "2gcB6DkQi6F7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "⚠️  Processing high missing column: neighborhood_slug\n",
            "Using RandomForestClassifier for high missing: neighborhood_slug\n"
          ]
        }
      ],
      "source": [
        "# Strategy 4: Handle high missing columns\n",
        "\n",
        "\n",
        "for col in high_missing:\n",
        "    print(f\"\\n⚠️  Processing high missing column: {col}\")\n",
        "    \n",
        "    missing_mask = df[col].isnull()\n",
        "    complete_data = df[~missing_mask].copy()\n",
        "    \n",
        "    if len(complete_data) < 50:  # Very few samples available\n",
        "        print(f\"Very few complete samples ({len(complete_data)}) for {col}, using simple imputation\")\n",
        "        \n",
        "        if col in categorical_columns or col in boolean_columns:\n",
        "            mode_val = complete_data[col].mode()[0] if len(complete_data) > 0 else df[col].mode()[0]\n",
        "            df.loc[missing_mask, col] = mode_val\n",
        "            print(f\"Used mode: {mode_val}\")\n",
        "        else:\n",
        "            median_val = complete_data[col].median() if len(complete_data) > 0 else df[col].median()\n",
        "            df.loc[missing_mask, col] = median_val\n",
        "            print(f\"Used median: {median_val:.2f}\")\n",
        "        continue\n",
        "    \n",
        "    if col in categorical_columns or col in boolean_columns:\n",
        "        print(f\"Using RandomForestClassifier for high missing: {col}\")\n",
        "        \n",
        "        features = [c for c in df.columns if c != col and df[c].isnull().mean() < 0.2]\n",
        "        X = complete_data[features]\n",
        "        y = complete_data[col]\n",
        "        \n",
        "        if len(X) < 100:\n",
        "            # Use cross-validation for small datasets\n",
        "            \n",
        "            model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "            scores = cross_val_score(model, X, y, cv=3, scoring='accuracy')\n",
        "            avg_acc = scores.mean()\n",
        "            print(f\"Cross-val accuracy for {col}: {avg_acc:.4f}\")\n",
        "            \n",
        "            if avg_acc >= 0.7:  # Lower threshold for high missing\n",
        "                model.fit(X, y)\n",
        "                missing_data = df[missing_mask][features].copy()\n",
        "                predictions = model.predict(missing_data)\n",
        "                df.loc[missing_mask, col] = predictions\n",
        "                print(f\"✅ Imputed with accuracy: {avg_acc:.4f}\")\n",
        "            else:\n",
        "                mode_val = complete_data[col].mode()[0]\n",
        "                df.loc[missing_mask, col] = mode_val\n",
        "                print(f\"Used mode fallback: {mode_val}\")\n",
        "        else:\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "            model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "            model.fit(X_train, y_train)\n",
        "            \n",
        "            y_pred = model.predict(X_test)\n",
        "            acc = accuracy_score(y_test, y_pred)\n",
        "            print(f\"Accuracy for {col}: {acc:.4f}\")\n",
        "            \n",
        "            if acc >= 0.7:\n",
        "                missing_data = df[missing_mask][features].copy()\n",
        "                predictions = model.predict(missing_data)\n",
        "                df.loc[missing_mask, col] = predictions\n",
        "                print(f\"✅ Imputed with accuracy: {acc:.4f}\")\n",
        "            else:\n",
        "                mode_val = complete_data[col].mode()[0]\n",
        "                df.loc[missing_mask, col] = mode_val\n",
        "                print(f\"Used mode fallback: {mode_val}\")\n",
        "    \n",
        "    elif col in e:  # Continuous price columns\n",
        "        print(f\"Using RandomForestRegressor for high missing price: {col}\")\n",
        "        \n",
        "        features = [c for c in df.columns if c != col and df[c].isnull().mean() < 0.2]\n",
        "        X = complete_data[features]\n",
        "        y = complete_data[col]\n",
        "        \n",
        "        if len(X) < 100:\n",
        "            from sklearn.model_selection import cross_val_score\n",
        "            model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
        "            scores = cross_val_score(model, X, y, cv=3, scoring='r2')\n",
        "            avg_r2 = scores.mean()\n",
        "            print(f\"Cross-val R2 for {col}: {avg_r2:.4f}\")\n",
        "            \n",
        "            if avg_r2 >= 0.6:\n",
        "                model.fit(X, y)\n",
        "                missing_data = df[missing_mask][features].copy()\n",
        "                predictions = model.predict(missing_data)\n",
        "                df.loc[missing_mask, col] = predictions\n",
        "                print(f\"✅ Imputed with R2: {avg_r2:.4f}\")\n",
        "            else:\n",
        "                median_val = complete_data[col].median()\n",
        "                df.loc[missing_mask, col] = median_val\n",
        "                print(f\"Used median fallback: {median_val:.2f}\")\n",
        "        else:\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "            model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
        "            model.fit(X_train, y_train)\n",
        "            \n",
        "            y_pred = model.predict(X_test)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "            print(f\"R2 for {col}: {r2:.4f}\")\n",
        "            \n",
        "            if r2 >= 0.6:\n",
        "                missing_data = df[missing_mask][features].copy()\n",
        "                predictions = model.predict(missing_data)\n",
        "                df.loc[missing_mask, col] = predictions\n",
        "                print(f\"✅ Imputed with R2: {r2:.4f}\")\n",
        "            else:\n",
        "                median_val = complete_data[col].median()\n",
        "                df.loc[missing_mask, col] = median_val\n",
        "                print(f\"Used median fallback: {median_val:.2f}\")\n",
        "    \n",
        "    else:  # Other numerical columns\n",
        "        print(f\"Using RandomForestRegressor for high missing numerical: {col}\")\n",
        "        \n",
        "        features = [c for c in df.columns if c != col and df[c].isnull().mean() < 0.2]\n",
        "        X = complete_data[features]\n",
        "        y = complete_data[col]\n",
        "        \n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "        y_pred = model.predict(X_test)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        print(f\"R2 for {col}: {r2:.4f}\")\n",
        "        \n",
        "        if r2 >= 0.6:\n",
        "            missing_data = df[missing_mask][features].copy()\n",
        "            predictions = model.predict(missing_data)\n",
        "            df.loc[missing_mask, col] = predictions\n",
        "            print(f\"✅ Imputed with R2: {r2:.4f}\")\n",
        "        else:\n",
        "            median_val = complete_data[col].median()\n",
        "            df.loc[missing_mask, col] = median_val\n",
        "            print(f\"Used median fallback: {median_val:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "637d2736",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# STEP 5:  FEATURE ENGINEERING\n",
        "# ============================================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83a03db5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time-based features from Persian date\n",
        "if 'created_at_month' in df.columns:\n",
        "    df['created_at_month'] = pd.to_datetime(\n",
        "        df['created_at_month'], errors='coerce')\n",
        "    \n",
        "    # Convert to JalaliDate and extract month directly\n",
        "    df['shamsi_date'] = df['created_at_month'].apply(\n",
        "        lambda x: JalaliDate(x) if pd.notna(x) else None)\n",
        "    \n",
        "    \n",
        "    df['month_created'] = df['shamsi_date'].apply(\n",
        "        lambda x: x.month if x is not None else None\n",
        "        \n",
        "    ).astype('Int8')\n",
        "    \n",
        "    df['year_created'] = df['shamsi_date'].apply(\n",
        "        lambda x: x.year if x is not None else None\n",
        "    ).astype('Int16')\n",
        "    \n",
        "    # Create season from month\n",
        "    df['season_created'] = ((df['month_created'] - 1) // 3 + 1).astype('Int8')    \n",
        "    \n",
        "   \n",
        "    df.drop('created_at_month', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4dd5b47",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Building age and condition features\n",
        "if 'construction_year' in df.columns:\n",
        "    current_year = 1403  # Current Shamsi year\n",
        "    df['building_age'] = (current_year - df['construction_year']).clip(0, 100).astype('Int8')\n",
        "   \n",
        "    # Age categories for non-linear relationships\n",
        "    df['age_category'] = pd.cut(df['building_age'],\n",
        "                               bins=[0, 2, 5, 9, 15, 100],\n",
        "                               labels=['New', 'Modern', 'Mature', 'Old', 'Very_Old']).astype(str)\n",
        "\n",
        "    # Building condition score based on age\n",
        "    df['condition_score'] = np.where(df['building_age'] <= 2, 5,\n",
        "                           np.where(df['building_age'] <= 5, 4,\n",
        "                           np.where(df['building_age'] <= 9, 3,\n",
        "                           np.where(df['building_age'] <= 50, 2, 1)))).astype('Int8')\n",
        "    \n",
        "    df.drop('construction_year', axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "\n",
        "    # Encode age_category with ordinal mapping\n",
        "    if 'age_category' in df.columns:\n",
        "        age_category_mapping = {'New': 5, 'Modern': 4, 'Mature': 3, 'Old': 2, 'Very_Old': 1}\n",
        "        df['age_category_encoded'] = df['age_category'].map(age_category_mapping)\n",
        "        df['age_category_encoded'].fillna(df['age_category_encoded'].median(), inplace=True)\n",
        "        df['age_category_encoded'] = df['age_category_encoded'].astype('int8')\n",
        "        df.drop('age_category', axis=1, inplace=True, errors='ignore')    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef2517e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Size and space utilization features\n",
        "if all(col in df.columns for col in ['building_size', 'land_size']):\n",
        "    df['building_to_land_ratio'] = (df['building_size'] / (df['land_size'] + 1)).clip(0, 1).astype('float32')\n",
        "    df['unused_land'] = (df['land_size'] - df['building_size']).clip(0).astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4800ab8",
      "metadata": {},
      "outputs": [],
      "source": [
        "if all(col in df.columns for col in ['rooms_count', 'building_size']):\n",
        "    df['room_size_avg'] = (df['building_size'] / (df['rooms_count'] + 1)).astype('float32')\n",
        "    df['room_density'] = (df['rooms_count'] / (df['building_size'] + 1)).astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "847b63f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Floor and position features\n",
        "if all(col in df.columns for col in ['floor', 'total_floors_count']):\n",
        "    df['floor_ratio'] = (df['floor'] / (df['total_floors_count'] + 1)).clip(0, 1).astype('float32')\n",
        "    df['is_ground_floor'] = (df['floor'] == 0).astype('int8')\n",
        "    df['is_top_floor'] = (df['floor'] == df['total_floors_count']).astype('int8')\n",
        "    df['is_middle_floor'] = ((df['floor'] > 0) & (df['floor'] < df['total_floors_count'])).astype('int8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3c79e2c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Price efficiency metrics\n",
        "for price_col in ['transformable_price', 'price_value']:\n",
        "    if price_col in df.columns and 'building_size' in df.columns:\n",
        "        df[f'{price_col}_per_sqm'] = (df[price_col] / (df['building_size'] + 1)).astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12b1c0ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "for rent_col in ['transformable_rent', 'rent_value']:\n",
        "    if rent_col in df.columns and 'building_size' in df.columns:\n",
        "        df[f'{rent_col}_per_sqm'] = (df[rent_col] / (df['building_size'] + 1)).astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5024067",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rental yield calculation (if both price and rent available)\n",
        "if all(col in df.columns for col in ['transformable_price', 'transformable_rent']):\n",
        "    df['rental_yield'] = (df['transformable_rent'] * 12 / (df['transformable_price'] + 1) * 100).clip(0, 50).astype('float32')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4e18531",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Amenities scoring system\n",
        "luxury_amenities = ['has_pool', 'has_jacuzzi', 'has_sauna', 'has_barbecue']\n",
        "comfort_amenities = ['has_elevator', 'has_parking', 'has_balcony', 'has_warehouse']\n",
        "basic_amenities = ['has_water', 'has_gas', 'has_electricity']\n",
        "security_amenities = ['has_security_guard']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ed25e08",
      "metadata": {},
      "outputs": [],
      "source": [
        "for amenity_group, name in [(luxury_amenities, 'luxury'),\n",
        "                           (comfort_amenities, 'comfort'),\n",
        "                           (basic_amenities, 'basic'),\n",
        "                           (security_amenities, 'security')]:\n",
        "    available_amenities = [col for col in amenity_group if col in df.columns]\n",
        "    if available_amenities:\n",
        "        df[f'{name}_score'] = df[available_amenities].sum(axis=1).astype('int8')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62d7c145",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Total amenities score with weights\n",
        "amenity_weights = {'luxury_score': 3, 'comfort_score': 2, 'basic_score': 1, 'security_score': 2}\n",
        "df['total_amenity_score'] = 0\n",
        "for score_col, weight in amenity_weights.items():\n",
        "    if score_col in df.columns:\n",
        "        df['total_amenity_score'] += df[score_col] * weight\n",
        "df['total_amenity_score'] = df['total_amenity_score'].astype('Int8')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b854322",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Location desirability (based on neighborhood price statistics)\n",
        "if 'neighborhood_slug' in df.columns and 'transformable_price' in df.columns:\n",
        "    neighborhood_stats = df.groupby('neighborhood_slug')['transformable_price'].agg(['mean', 'median', 'count'])\n",
        "    neighborhood_stats['price_rank'] = neighborhood_stats['median'].rank(pct=True)\n",
        "    df = df.merge(neighborhood_stats[['price_rank']].reset_index(), on='neighborhood_slug', how='left')\n",
        "    df['neighborhood_desirability'] = (df['price_rank'] * 5).round().astype('int8')\n",
        "    df.drop('price_rank', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b2b40c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Market segment classification\n",
        "if 'transformable_price' in df.columns:\n",
        "    # Ensure numeric\n",
        "    df['transformable_price'] = pd.to_numeric(df['transformable_price'], errors='coerce')\n",
        "\n",
        "    # Drop NaNs for quantile calculation\n",
        "    valid_prices = df['transformable_price'].dropna()\n",
        "    if not valid_prices.empty:\n",
        "        price_quartiles = valid_prices.quantile([0.25, 0.5, 0.75])\n",
        "\n",
        "        # Define bins safely\n",
        "        bins = [0] + price_quartiles.tolist() + [float('inf')]\n",
        "\n",
        "        # Ensure bins are strictly increasing\n",
        "        bins = sorted(set(bins))\n",
        "\n",
        "        # Create labels dynamically\n",
        "        labels = ['Budget', 'Mid_Range', 'Premium', 'Luxury'][:len(bins)-1]\n",
        "\n",
        "        df['market_segment'] = pd.cut(\n",
        "            df['transformable_price'],\n",
        "            bins=bins,\n",
        "            labels=labels,\n",
        "            duplicates='drop'   # handle duplicate edges safely\n",
        "        ).astype(str)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef12f3bd",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# STEP 6:  CATEGORICAL ENCODING\n",
        "# ============================================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "le75kiV5k96_",
      "metadata": {
        "id": "le75kiV5k96_"
      },
      "outputs": [],
      "source": [
        "# Identify categorical columns\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Target encoding for high-cardinality location columns (if target available)\n",
        "high_card_cols = ['city_slug', 'neighborhood_slug']\n",
        "target_col = 'transformable_price'  # Primary target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "3KB8_DHmlAvA",
      "metadata": {
        "id": "3KB8_DHmlAvA"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'target_col' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m target_col \u001b[38;5;129;01min\u001b[39;00m df.columns:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m high_card_cols:\n\u001b[32m      3\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df.columns \u001b[38;5;129;01mand\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m categorical_cols:\n\u001b[32m      4\u001b[39m             \u001b[38;5;66;03m# Calculate mean target value for each category\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'target_col' is not defined"
          ]
        }
      ],
      "source": [
        "if target_col in df.columns:\n",
        "    for col in high_card_cols:\n",
        "        if col in df.columns and col in categorical_cols:\n",
        "            # Calculate mean target value for each category\n",
        "            target_mean = df.groupby(col)[target_col].mean()\n",
        "            global_mean = df[target_col].mean()\n",
        "\n",
        "            # Add smoothing to prevent overfitting\n",
        "            category_counts = df[col].value_counts()\n",
        "            smoothing = 100  # Smoothing parameter\n",
        "            smoothed_means = (target_mean * category_counts + global_mean * smoothing) / (category_counts + smoothing)\n",
        "\n",
        "            df[f'{col}_target_encoded'] = df[col].map(smoothed_means).fillna(global_mean).astype('float32')\n",
        "            categorical_cols.remove(col) if col in categorical_cols else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1oPhe_HMlEHh",
      "metadata": {
        "id": "1oPhe_HMlEHh"
      },
      "outputs": [],
      "source": [
        "# Label encoding for remaining categorical columns\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    if col in df.columns:\n",
        "        le = LabelEncoder()\n",
        "        # Handle missing values\n",
        "        df[col] = df[col].astype(str).fillna('Unknown')\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "        label_encoders[col] = le\n",
        "\n",
        "        # Convert to appropriate integer type\n",
        "        if len(le.classes_) < 128:\n",
        "            df[col] = df[col].astype('Int8')\n",
        "        elif len(le.classes_) < 32768:\n",
        "            df[col] = df[col].astype('Int16')\n",
        "\n",
        "print(f\"Encoded {len(categorical_cols)} categorical columns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bxO0DEhklGeE",
      "metadata": {
        "id": "bxO0DEhklGeE"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STEP 7: FEATURE SCALING AND NORMALIZATION\n",
        "# ============================================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Vnu1YnaBlKg6",
      "metadata": {
        "id": "Vnu1YnaBlKg6"
      },
      "outputs": [],
      "source": [
        "# Identify numerical columns for scaling\n",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "price_cols_in_df = [col for col in price_columns if col in numerical_cols]\n",
        "size_cols_in_df = [col for col in size_columns if col in numerical_cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcRM_Z2xlipL",
      "metadata": {
        "id": "fcRM_Z2xlipL"
      },
      "outputs": [],
      "source": [
        "# Different scaling strategies for different types of features\n",
        "scalers = {}\n",
        "\n",
        "# Log transformation for highly skewed price data\n",
        "for col in price_cols_in_df:\n",
        "    if df[col].min() > 0:  # Ensure all values are positive\n",
        "        df[f'{col}_log'] = np.log1p(df[col]).astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U1uIRqfDlmcj",
      "metadata": {
        "id": "U1uIRqfDlmcj"
      },
      "outputs": [],
      "source": [
        "# Power transformation for moderately skewed data\n",
        "power_transformer = PowerTransformer(method='yeo-johnson', standardize=True)\n",
        "skewed_cols = []\n",
        "for col in numerical_cols:\n",
        "    if col not in price_cols_in_df and df[col].dtype in ['float32', 'float64']:\n",
        "        skewness = abs(df[col].skew())\n",
        "        if skewness > 1:  # Moderately skewed\n",
        "            skewed_cols.append(col)\n",
        "\n",
        "if skewed_cols:\n",
        "    df_skewed_transformed = power_transformer.fit_transform(df[skewed_cols])\n",
        "    for i, col in enumerate(skewed_cols):\n",
        "        df[f'{col}_transformed'] = df_skewed_transformed[:, i].astype('float32')\n",
        "    scalers['power_transformer'] = power_transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f-8vgDKZlpp6",
      "metadata": {
        "id": "f-8vgDKZlpp6"
      },
      "outputs": [],
      "source": [
        "# Robust scaling for price and size features (resistant to outliers)\n",
        "robust_scaler = RobustScaler()\n",
        "robust_cols = price_cols_in_df + size_cols_in_df + [col for col in numerical_cols if 'per_sqm' in col]\n",
        "robust_cols = [col for col in robust_cols if col in df.columns]\n",
        "\n",
        "if robust_cols:\n",
        "    df_robust_scaled = robust_scaler.fit_transform(df[robust_cols])\n",
        "    for i, col in enumerate(robust_cols):\n",
        "        df[f'{col}_robust_scaled'] = df_robust_scaled[:, i].astype('float32')\n",
        "    scalers['robust_scaler'] = robust_scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AK7gRWY5ls9F",
      "metadata": {
        "id": "AK7gRWY5ls9F"
      },
      "outputs": [],
      "source": [
        "# Standard scaling for normally distributed features\n",
        "normal_cols = [col for col in numerical_cols\n",
        "               if col not in robust_cols and col not in skewed_cols\n",
        "               and col not in price_cols_in_df\n",
        "               and df[col].dtype in ['float32', 'float64']\n",
        "               and abs(df[col].skew()) <= 1]\n",
        "\n",
        "if normal_cols:\n",
        "    standard_scaler = StandardScaler()\n",
        "    df_standard_scaled = standard_scaler.fit_transform(df[normal_cols])\n",
        "    for i, col in enumerate(normal_cols):\n",
        "        df[f'{col}_standard_scaled'] = df_standard_scaled[:, i].astype('float32')\n",
        "    scalers['standard_scaler'] = standard_scaler\n",
        "\n",
        "print(f\"Applied scaling to {len(robust_cols + normal_cols + skewed_cols)} numerical features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5hd6dGdlw0h",
      "metadata": {
        "id": "d5hd6dGdlw0h"
      },
      "outputs": [],
      "source": [
        "\n",
        "# =============================================================================\n",
        "# STEP 8: FEATURE SELECTION AND DIMENSIONALITY REDUCTION\n",
        "# ============================================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wDby4jQVl31c",
      "metadata": {
        "id": "wDby4jQVl31c"
      },
      "outputs": [],
      "source": [
        "# Remove highly correlated features to reduce multicollinearity\n",
        "correlation_threshold = 0.85\n",
        "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "if len(numerical_features) > 1:\n",
        "    corr_matrix = df[numerical_features].corr().abs()\n",
        "    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "    # Find highly correlated feature pairs\n",
        "    high_corr_pairs = [(corr_matrix.index[i], corr_matrix.columns[j])\n",
        "                       for i in range(len(corr_matrix.index))\n",
        "                       for j in range(len(corr_matrix.columns))\n",
        "                       if upper_triangle.iloc[i, j] > correlation_threshold]\n",
        "\n",
        "    # Remove features with high correlation (keep the first one in each pair)\n",
        "    features_to_drop = []\n",
        "    for feature1, feature2 in high_corr_pairs:\n",
        "        if feature2 not in features_to_drop:\n",
        "            features_to_drop.append(feature2)\n",
        "\n",
        "    if features_to_drop:\n",
        "        df.drop(columns=features_to_drop, inplace=True)\n",
        "        print(f\"Removed {len(features_to_drop)} highly correlated features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "weAeTevkl8ek",
      "metadata": {
        "id": "weAeTevkl8ek"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STEP 9: MEMORY OPTIMIZATION\n",
        "# ============================================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZdNoRiUIl_zW",
      "metadata": {
        "id": "ZdNoRiUIl_zW"
      },
      "outputs": [],
      "source": [
        "# Optimize data types\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'float64':\n",
        "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
        "    elif df[col].dtype == 'int64':\n",
        "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
        "\n",
        "# Convert low-cardinality object columns to category\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    if df[col].nunique() < len(df) * 0.3:  # Less than 30% unique values\n",
        "        df[col] = df[col].astype('category')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fi9Zbgl5mCpj",
      "metadata": {
        "id": "fi9Zbgl5mCpj"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STEP 10: FINAL VALIDATION AND EXPORT\n",
        "# ============================================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o8hMx7-9mGRS",
      "metadata": {
        "id": "o8hMx7-9mGRS"
      },
      "outputs": [],
      "source": [
        "# Ensure no missing values remain\n",
        "remaining_missing = df.isnull().sum().sum()\n",
        "if remaining_missing > 0:\n",
        "    print(f\"WARNING: {remaining_missing} missing values remain!\")\n",
        "    # Fill any remaining missing values\n",
        "    for col in df.columns:\n",
        "        if df[col].isnull().sum() > 0:\n",
        "            if df[col].dtype in ['float16', 'float32', 'float64']:\n",
        "                df[col] = df[col].fillna(df[col].median())\n",
        "            else:\n",
        "                df[col] = df[col].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MfxIm02NmJOz",
      "metadata": {
        "id": "MfxIm02NmJOz"
      },
      "outputs": [],
      "source": [
        "# Validate data integrity\n",
        "assert df.isnull().sum().sum() == 0, \"Missing values still exist!\"\n",
        "assert df.shape[0] > 0, \"No data remaining!\"\n",
        "assert not df.duplicated().any(), \"Duplicate rows exist!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VuIFAR3ImSFb",
      "metadata": {
        "id": "VuIFAR3ImSFb"
      },
      "outputs": [],
      "source": [
        "# Export processed data\n",
        "df.to_csv(\"real_estate_ml_ready.csv\", index=False)\n",
        "df.to_parquet(\"real_estate_ml_ready.parquet\", index=False)\n",
        "df.to_pickle(\"real_estate_ml_ready.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86V_mYBlmXlE",
      "metadata": {
        "id": "86V_mYBlmXlE"
      },
      "outputs": [],
      "source": [
        "# Export preprocessing artifacts\n",
        "import pickle\n",
        "with open('preprocessing_artifacts.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'label_encoders': label_encoders,\n",
        "        'scalers': scalers,\n",
        "        'feature_columns': df.columns.tolist(),\n",
        "        'outlier_summary': outlier_summary\n",
        "    }, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QtxESW4dmbWW",
      "metadata": {
        "id": "QtxESW4dmbWW"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COMPREHENSIVE SUMMARY\n",
        "# ============================================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ROhnKMORmiMF",
      "metadata": {
        "id": "ROhnKMORmiMF"
      },
      "outputs": [],
      "source": [
        "end_time = datetime.now()\n",
        "processing_time = (end_time - start_time).total_seconds()\n",
        "final_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
        "memory_reduction = ((original_memory - final_memory) / original_memory) * 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MbGHnaUnmmjk",
      "metadata": {
        "id": "MbGHnaUnmmjk"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ADVANCED PREPROCESSING COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Original shape: {original_shape[0]:,} × {original_shape[1]}\")\n",
        "print(f\"Final shape: {df.shape[0]:,} × {df.shape[1]}\")\n",
        "print(f\"Features created: {df.shape[1] - original_shape[1]}\")\n",
        "print(f\"Memory usage: {original_memory:.1f} MB → {final_memory:.1f} MB ({memory_reduction:.1f}% reduction)\")\n",
        "print(f\"Processing time: {processing_time:.1f} seconds\")\n",
        "print(f\"Missing values: {remaining_missing}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1LU73gWzmpJw",
      "metadata": {
        "id": "1LU73gWzmpJw"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(f\"\\nData Quality Improvements:\")\n",
        "print(f\"  ✓ Advanced outlier detection and capping\")\n",
        "print(f\"  ✓ Multi-strategy missing value imputation\")\n",
        "print(f\"  ✓ Sophisticated feature engineering ({df.shape[1] - original_shape[1]} new features)\")\n",
        "print(f\"  ✓ Domain-specific real estate transformations\")\n",
        "print(f\"  ✓ Multi-method outlier handling\")\n",
        "print(f\"  ✓ Advanced categorical encoding (target + label)\")\n",
        "print(f\"  ✓ Intelligent feature scaling (robust + standard + power)\")\n",
        "print(f\"  ✓ Multicollinearity reduction\")\n",
        "print(f\"  ✓ Memory optimization\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "osc0akXumuO-",
      "metadata": {
        "id": "osc0akXumuO-"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nKey Real Estate Features Created:\")\n",
        "new_features = [\n",
        "    'building_age', 'age_category', 'condition_score',\n",
        "    'building_to_land_ratio', 'room_size_avg', 'room_density',\n",
        "    'floor_ratio', 'is_ground_floor', 'is_top_floor',\n",
        "    'luxury_score', 'comfort_score', 'total_amenity_score',\n",
        "    'neighborhood_desirability', 'market_segment', 'rental_yield'\n",
        "]\n",
        "\n",
        "for feature in new_features:\n",
        "    if feature in df.columns:\n",
        "        print(f\"  ✓ {feature}\")\n",
        "\n",
        "print(f\"\\nData Types Summary:\")\n",
        "dtype_counts = df.dtypes.value_counts()\n",
        "for dtype, count in dtype_counts.items():\n",
        "    print(f\"  {dtype}: {count} columns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0_wghpX_mxKT",
      "metadata": {
        "id": "0_wghpX_mxKT"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nTop Features by Type:\")\n",
        "price_features = [col for col in df.columns if 'price' in col.lower() or 'rent' in col.lower()]\n",
        "size_features = [col for col in df.columns if 'size' in col.lower() or 'area' in col.lower()]\n",
        "location_features = [col for col in df.columns if any(x in col.lower() for x in ['city', 'neighborhood', 'location'])]\n",
        "amenity_features = [col for col in df.columns if 'has_' in col.lower() or 'score' in col.lower()]\n",
        "\n",
        "print(f\"  Price-related: {len(price_features)} features\")\n",
        "print(f\"  Size-related: {len(size_features)} features\")\n",
        "print(f\"  Location-related: {len(location_features)} features\")\n",
        "print(f\"  Amenity-related: {len(amenity_features)} features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x7zWP7NSm7ko",
      "metadata": {
        "id": "x7zWP7NSm7ko"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nReady for Machine Learning:\")\n",
        "print(f\"  ✓ No missing values\")\n",
        "print(f\"  ✓ Optimized data types\")\n",
        "print(f\"  ✓ Scaled numerical features\")\n",
        "print(f\"  ✓ Encoded categorical features\")\n",
        "print(f\"  ✓ Rich feature set for real estate prediction\")\n",
        "print(f\"  ✓ Preprocessing artifacts saved for production use\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "quera-project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
